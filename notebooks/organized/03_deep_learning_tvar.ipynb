{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a936b27c",
   "metadata": {},
   "source": [
    "# 03 — Deep Learning for TVAR Estimation\n",
    "\n",
    "This notebook implements neural network approaches for learning time-varying AR coefficients.\n",
    "\n",
    "## Approaches\n",
    "- **Neural ODE + TVAR**: ODE evolves latent → Levinson-Durbin → stable AR coeffs\n",
    "- **Lag-Attention (Transformer)**: Attention over lag bank, sparse top-k selection\n",
    "- **Neural Operator**: Continuous-τ kernel with Fourier features\n",
    "- **Transformer AR**: CLS token over lag sequence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846061e4",
   "metadata": {},
   "source": [
    "## 1. Neural ODE + TVAR (Levinson-Durbin Stability)\n",
    "\n",
    "**Source**: `NeuralODE.ipynb`\n",
    "\n",
    "**Key Idea**: A Neural ODE evolves a latent state `z(t)` over time. The latent is mapped through `tanh` to obtain reflection coefficients $\\kappa \\in (-1, 1)$, which are then converted to stable AR(p) coefficients via Levinson-Durbin recursion.\n",
    "\n",
    "$$\\kappa_i \\in (-1, 1) \\Rightarrow \\text{stable AR}$$\n",
    "\n",
    "**Components**:\n",
    "- `ODEFunc`: MLP that defines $dz/dt = f_\\theta(z)$\n",
    "- `Encoder`: Maps initial window of observations to $z_0$\n",
    "- `NeuralODE_TVAR2`: Integrates ODE (Euler), applies Levinson-Durbin\n",
    "\n",
    "**Levinson-Durbin for AR(2)**:\n",
    "$$a_2 = \\kappa_2, \\quad a_1 = \\kappa_1 (1 - \\kappa_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Neural ODE + TVAR (from NeuralODE.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "def levinson_order2(kappa):\n",
    "    \"\"\"\n",
    "    Levinson-Durbin recursion for AR(2) from reflection coefficients.\n",
    "    \n",
    "    Maps reflection coefficients κ ∈ (-1, 1) to stable AR coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kappa : Tensor, shape (..., 2)\n",
    "        Reflection coefficients (use tanh to constrain to (-1, 1))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : Tensor, shape (..., 2)\n",
    "        AR(2) coefficients [a1, a2] guaranteed to be stable\n",
    "    \"\"\"\n",
    "    k1 = kappa[..., 0]\n",
    "    k2 = kappa[..., 1]\n",
    "    a2 = k2\n",
    "    a1 = k1 * (1.0 - k2)\n",
    "    return torch.stack([a1, a2], dim=-1)\n",
    "\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the ODE dynamics dz/dt = f_θ(z).\n",
    "    \n",
    "    A simple MLP that maps z → dz/dt.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=2, hidden=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps an initial window of observations to the initial latent state z0.\n",
    "    \"\"\"\n",
    "    def __init__(self, L=30, hidden=32, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(L, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        return self.net(x0)\n",
    "\n",
    "\n",
    "class NeuralODE_TVAR2(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural ODE for Time-Varying AR(2) estimation.\n",
    "    \n",
    "    The latent state z(t) evolves via an ODE, then is mapped to stable\n",
    "    AR coefficients using Levinson-Durbin recursion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        Window size for encoding z0\n",
    "    hidden : int\n",
    "        Hidden dimension for ODE function\n",
    "    \n",
    "    Forward\n",
    "    -------\n",
    "    x_seq : Tensor, shape (T,)\n",
    "        Observed time series\n",
    "    phi_seq : Tensor, shape (T, 2)\n",
    "        Lagged features [x_{t-1}, x_{t-2}]\n",
    "    dt : float\n",
    "        Integration time step\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xhat : Tensor, shape (T,)\n",
    "        One-step predictions (teacher-forced)\n",
    "    a : Tensor, shape (T, 2)\n",
    "        Time-varying AR coefficients\n",
    "    z : Tensor, shape (T, 2)\n",
    "        Latent ODE trajectory\n",
    "    \"\"\"\n",
    "    def __init__(self, L=30, hidden=32):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.func = ODEFunc(dim=2, hidden=hidden)\n",
    "        self.enc = Encoder(L=L, hidden=hidden, out_dim=2)\n",
    "\n",
    "    def forward(self, x_seq, phi_seq, dt=1.0):\n",
    "        T = x_seq.shape[0]\n",
    "        z0 = self.enc(x_seq[:self.L].unsqueeze(0)).squeeze(0)  # (2,)\n",
    "\n",
    "        # Euler integration of ODE\n",
    "        z_list = [z0]\n",
    "        for _ in range(1, T):\n",
    "            z_prev = z_list[-1]\n",
    "            z_next = z_prev + dt * self.func(z_prev)\n",
    "            z_list.append(z_next)\n",
    "        z = torch.stack(z_list, dim=0)  # (T, 2)\n",
    "\n",
    "        # Map to stable AR coefficients\n",
    "        kappa = torch.tanh(z)          # reflection coeffs in (-1, 1)\n",
    "        a = levinson_order2(kappa)     # stable AR(2) coeffs\n",
    "\n",
    "        # One-step prediction\n",
    "        xhat = torch.zeros(T, device=x_seq.device)\n",
    "        xhat[2:] = (a[2:] * phi_seq[2:]).sum(dim=1)\n",
    "        \n",
    "        return xhat, a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08c14c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Lag-Attention TVAR (Transformer-based)\n",
    "\n",
    "**Source**: `DeepLagAttention.ipynb`\n",
    "\n",
    "**Key Idea**: Build a \"lag bank\" of past values $[x_{t-1}, x_{t-2}, \\ldots, x_{t-L}]$ and use transformer attention to learn which lags are important at each time step. A sparse top-k selection focuses on the most relevant lags.\n",
    "\n",
    "**Prediction**:\n",
    "$$\\hat{x}_t = \\sum_{\\ell=1}^{L} w_\\ell(t) \\cdot x_{t-\\ell} + c(t)$$\n",
    "\n",
    "where $w(t)$ is a learned, time-varying attention distribution over lags.\n",
    "\n",
    "**Two Variants**:\n",
    "1. `LagAttentionTVAR` — Full transformer encoder over lag tokens\n",
    "2. `LagAttentionTVARFast` — Bilinear scoring (no transformer, faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84838e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Lag-Attention TVAR (from DeepLagAttention.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def build_lag_bank(x_tensor, L):\n",
    "    \"\"\"\n",
    "    Build lag bank from time series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_tensor : Tensor, shape (B, T)\n",
    "        Input time series\n",
    "    L : int\n",
    "        Number of lags\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Xlags : Tensor, shape (B, T, L)\n",
    "        Xlags[:, t, l] = x_{t-(l+1)}, left-padded with zeros\n",
    "    \"\"\"\n",
    "    B, T = x_tensor.shape\n",
    "    pads = F.pad(x_tensor, (L, 0))  # [B, T+L]\n",
    "    idx_base = torch.arange(T, device=x_tensor.device).view(1, T, 1)\n",
    "    lag_offsets = torch.arange(L, device=x_tensor.device).view(1, 1, L)\n",
    "    gather_idx = L - 1 + idx_base + lag_offsets\n",
    "    Xlags = pads[:, gather_idx.squeeze(0)]\n",
    "    return Xlags\n",
    "\n",
    "\n",
    "def topk_mask_logits(logits, k):\n",
    "    \"\"\"Mask all but top-k logits with -inf for sparse softmax.\"\"\"\n",
    "    if k >= logits.shape[-1]:\n",
    "        return logits\n",
    "    topk = torch.topk(logits, k, dim=-1)\n",
    "    mask = torch.full_like(logits, float('-inf'))\n",
    "    return mask.scatter(-1, topk.indices, topk.values)\n",
    "\n",
    "\n",
    "class LagAttentionTVAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Lag-Attention TVAR with full Transformer encoder.\n",
    "    \n",
    "    Learns time-varying attention weights over a bank of lagged values.\n",
    "    Uses sparse top-k selection for efficiency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        Number of lags in the bank\n",
    "    d_model : int\n",
    "        Model dimension\n",
    "    n_layers : int\n",
    "        Number of transformer layers\n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "    topk : int\n",
    "        Top-k sparsity in attention\n",
    "    use_var : bool\n",
    "        If True, also predict log-variance for NLL loss\n",
    "    \"\"\"\n",
    "    def __init__(self, L=256, d_model=128, n_layers=2, n_heads=4, topk=8, use_var=False):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.topk = topk\n",
    "        self.use_var = use_var\n",
    "\n",
    "        # Embeddings for lag indices and values\n",
    "        self.lag_embed = nn.Embedding(L + 1, d_model)\n",
    "        self.val_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        # Transformer encoder for lag tokens\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, n_heads,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.lag_encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # Causal context (conv over past)\n",
    "        self.ctx_conv = nn.Conv1d(1, d_model, kernel_size=9, padding=8, dilation=1)\n",
    "        self.ctx_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Scoring: pair(lag_enc, context) → scalar\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.bias_head = nn.Linear(d_model, 1)\n",
    "        if use_var:\n",
    "            self.logvar_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (B, T)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor, shape (B, T)\n",
    "            Predicted mean\n",
    "        logvar : Tensor or None\n",
    "            Log-variance (if use_var=True)\n",
    "        w : Tensor, shape (B, T, L)\n",
    "            Attention weights over lags\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        L = self.L\n",
    "\n",
    "        Xlags = build_lag_bank(x, L)  # [B, T, L]\n",
    "        lag_vals = Xlags.unsqueeze(-1)  # [B, T, L, 1]\n",
    "        lag_ids = torch.arange(1, L + 1, device=x.device).view(1, 1, L).expand(B, T, L)\n",
    "\n",
    "        Hval = self.val_proj(lag_vals).squeeze(-2)  # [B, T, L, d]\n",
    "        Hidx = self.lag_embed(lag_ids)  # [B, T, L, d]\n",
    "        Hlag = Hval + Hidx  # [B, T, L, d]\n",
    "\n",
    "        # Encode lag tokens per time step\n",
    "        Hlag_flat = Hlag.view(B * T, L, -1)\n",
    "        Henc = self.lag_encoder(Hlag_flat).view(B, T, L, -1)  # [B, T, L, d]\n",
    "\n",
    "        # Causal context from x\n",
    "        ctx = self.ctx_conv(x.unsqueeze(1))  # [B, d, T+pad]\n",
    "        ctx = ctx[..., :T].transpose(1, 2)  # [B, T, d]\n",
    "        ctx = self.ctx_proj(ctx)  # [B, T, d]\n",
    "\n",
    "        # Score lags with context\n",
    "        ctx_exp = ctx.unsqueeze(2).expand(-1, -1, L, -1)\n",
    "        pair = torch.cat([Henc, ctx_exp], dim=-1)  # [B, T, L, 2d]\n",
    "        logits = self.score(pair).squeeze(-1)  # [B, T, L]\n",
    "\n",
    "        logits_masked = topk_mask_logits(logits, k=self.topk)\n",
    "        w = torch.softmax(logits_masked, dim=-1)  # [B, T, L]\n",
    "\n",
    "        mu_ar = (w * Xlags).sum(dim=-1)  # [B, T]\n",
    "        c = self.bias_head(ctx).squeeze(-1)  # [B, T]\n",
    "        mu = mu_ar + c\n",
    "\n",
    "        logvar = None\n",
    "        if self.use_var:\n",
    "            logvar = self.logvar_head(ctx).squeeze(-1).clamp(-8, 8)\n",
    "        \n",
    "        return mu, logvar, w\n",
    "\n",
    "\n",
    "class LagAttentionTVARFast(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast Lag-Attention TVAR (no Transformer, bilinear scoring).\n",
    "    \n",
    "    Uses bilinear attention: score = <Wq·ctx, Wk·Hlag> instead of\n",
    "    full transformer, making it much faster for long sequences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        Number of lags\n",
    "    d_model : int\n",
    "        Model dimension\n",
    "    topk : int\n",
    "        Top-k sparsity\n",
    "    use_var : bool\n",
    "        If True, predict log-variance\n",
    "    \"\"\"\n",
    "    def __init__(self, L=256, d_model=128, topk=8, use_var=False):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.topk = topk\n",
    "        self.use_var = use_var\n",
    "\n",
    "        self.lag_embed = nn.Embedding(L + 1, d_model)\n",
    "        self.val_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        # Causal context (left-padded conv)\n",
    "        self.ctx_pad = nn.ConstantPad1d((8, 0), 0)\n",
    "        self.ctx_conv = nn.Conv1d(1, d_model, kernel_size=9, padding=0)\n",
    "        self.ctx_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Bilinear scorer\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.bias_head = nn.Linear(d_model, 1)\n",
    "        if use_var:\n",
    "            self.logvar_head = nn.Linear(d_model, 1)\n",
    "\n",
    "        # Init for stability\n",
    "        for m in [self.Wq, self.Wk, self.val_proj, self.bias_head]:\n",
    "            if hasattr(m, 'weight'):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        L = self.L\n",
    "\n",
    "        Xlags = build_lag_bank(x, L)  # [B, T, L]\n",
    "        lag_vals = Xlags.unsqueeze(-1)  # [B, T, L, 1]\n",
    "        lag_ids = torch.arange(1, L + 1, device=x.device).view(1, 1, L).expand(B, T, L)\n",
    "\n",
    "        Hval = self.val_proj(lag_vals).squeeze(-2)  # [B, T, L, d]\n",
    "        Hidx = self.lag_embed(lag_ids)  # [B, T, L, d]\n",
    "        Hlag = Hval + Hidx  # [B, T, L, d]\n",
    "\n",
    "        # Causal context\n",
    "        ctx = self.ctx_conv(self.ctx_pad(x.unsqueeze(1)))  # [B, d, T]\n",
    "        ctx = ctx.transpose(1, 2)  # [B, T, d]\n",
    "        ctx = self.ctx_proj(ctx)  # [B, T, d]\n",
    "\n",
    "        # Bilinear scores\n",
    "        q = self.Wq(ctx)  # [B, T, d]\n",
    "        k = self.Wk(Hlag)  # [B, T, L, d]\n",
    "        logits = torch.einsum('btd,btld->btl', q, k)  # [B, T, L]\n",
    "\n",
    "        # Top-k masking\n",
    "        if (self.topk is not None) and (self.topk < L):\n",
    "            topk_vals = torch.topk(logits, self.topk, dim=-1)\n",
    "            mask = torch.full_like(logits, float('-inf'))\n",
    "            logits = mask.scatter(-1, topk_vals.indices, topk_vals.values)\n",
    "\n",
    "        w = torch.softmax(logits, dim=-1)  # [B, T, L]\n",
    "        mu_ar = (w * Xlags).sum(dim=-1)  # [B, T]\n",
    "        c = self.bias_head(ctx).squeeze(-1)  # [B, T]\n",
    "        mu = mu_ar + c\n",
    "\n",
    "        logvar = None\n",
    "        if self.use_var:\n",
    "            logvar = self.logvar_head(ctx).squeeze(-1).clamp(-8, 8)\n",
    "        \n",
    "        return mu, logvar, w\n",
    "\n",
    "\n",
    "def gaussian_nll(y, mu, logvar):\n",
    "    \"\"\"Gaussian negative log-likelihood loss.\"\"\"\n",
    "    if logvar is None:\n",
    "        logvar = torch.zeros_like(mu)\n",
    "    return 0.5 * (logvar + (y - mu)**2 / (logvar.exp() + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9c23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Transformer AR (Fixed Lag Order)\n",
    "\n",
    "**Source**: `fixed_linear_transformer.ipynb`\n",
    "\n",
    "**Key Idea**: Standard transformer encoder operating on a fixed-size lag sequence. Uses a CLS token to summarize the lag information and predict the next value.\n",
    "\n",
    "**Architecture**:\n",
    "- Input: $[x_{t-1}, x_{t-2}, \\ldots, x_{t-P}]$ as sequence of tokens\n",
    "- Learned positional embeddings for lag positions\n",
    "- CLS token prepended, then transformed\n",
    "- Output head predicts $\\hat{x}_t$ from CLS representation\n",
    "\n",
    "**Use Case**: When AR order is fixed but nonlinear lag interactions are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Transformer AR (from fixed_linear_transformer.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "class TransformerAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based AR model with fixed lag order.\n",
    "    \n",
    "    Uses a CLS token to summarize the lag sequence and predict the next value.\n",
    "    Suitable for learning nonlinear interactions between lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Pmax : int\n",
    "        Maximum lag order (sequence length)\n",
    "    d_model : int\n",
    "        Transformer model dimension\n",
    "    nhead : int\n",
    "        Number of attention heads\n",
    "    depth : int\n",
    "        Number of transformer layers\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, Pmax, d_model=64, nhead=4, depth=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.Pmax = Pmax\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Project scalar lag values to d_model\n",
    "        self.in_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        # Learned positional embeddings for lag positions 0..Pmax-1\n",
    "        self.pos = nn.Parameter(torch.zeros(1, Pmax, d_model))\n",
    "        nn.init.normal_(self.pos, mean=0.0, std=0.02)\n",
    "\n",
    "        # CLS token to summarize the sequence\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, mean=0.0, std=0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=depth)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_seq : Tensor, shape (B, Pmax, 1)\n",
    "            Lag sequence where position 0 is x_{t-1}, position 1 is x_{t-2}, etc.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        yhat : Tensor, shape (B,)\n",
    "            Predicted next value\n",
    "        \"\"\"\n",
    "        h = self.in_proj(X_seq) + self.pos  # (B, Pmax, d)\n",
    "        cls = self.cls.expand(X_seq.size(0), 1, -1)  # (B, 1, d)\n",
    "        h = torch.cat([cls, h], dim=1)  # (B, 1+Pmax, d)\n",
    "\n",
    "        h = self.encoder(h)  # (B, 1+Pmax, d)\n",
    "        h_cls = h[:, 0, :]  # (B, d) — CLS token output\n",
    "        yhat = self.out(h_cls).squeeze(-1)  # (B,)\n",
    "        \n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eded39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. MLP TVAR (Hyper-Network)\n",
    "\n",
    "**Source**: `tvar_linear_mlp.ipynb`\n",
    "\n",
    "**Key Idea**: A \"hyper-network\" MLP that takes the lag vector as input and outputs time-varying AR coefficients. The prediction is then a learned linear combination of lags.\n",
    "\n",
    "$$\\hat{x}_t = \\mathbf{a}(z_t)^\\top z_t + b(z_t)$$\n",
    "\n",
    "where $z_t = [x_{t-1}, \\ldots, x_{t-p}]$ and both $\\mathbf{a}$ and $b$ are produced by the MLP.\n",
    "\n",
    "**Architecture**:\n",
    "- Backbone: MLP with GELU activation, LayerNorm, Dropout\n",
    "- Head: Linear layer outputting $[a_1, \\ldots, a_p, b]$\n",
    "- Prediction: Dot product of learned coefficients with lags + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46577732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MLP TVAR — Hyper-Network (from tvar_linear_mlp.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "class MLPTVAR(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-based Time-Varying AR model (Hyper-Network approach).\n",
    "    \n",
    "    The MLP takes the lag vector z_t = [x_{t-1}, ..., x_{t-p}] and outputs\n",
    "    time-varying coefficients a(t) and bias b(t). Prediction is:\n",
    "    \n",
    "        x̂_t = a(z_t)ᵀ z_t + b(z_t)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_max : int\n",
    "        Maximum lag order\n",
    "    hidden : int\n",
    "        Hidden dimension\n",
    "    depth : int\n",
    "        Number of hidden layers\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, p_max, hidden=128, depth=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = p_max\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(in_dim, hidden))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.LayerNorm(hidden))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        # Output: [a_1, ..., a_p_max, b]\n",
    "        self.head = nn.Linear(hidden, p_max + 1)\n",
    "        self.p_max = p_max\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : Tensor, shape (B, p_max)\n",
    "            Lag features [x_{t-1}, ..., x_{t-p}]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pred : Tensor, shape (B,)\n",
    "            Predicted next value\n",
    "        coeffs : Tensor, shape (B, p_max)\n",
    "            Learned time-varying AR coefficients\n",
    "        bias : Tensor, shape (B,)\n",
    "            Learned time-varying bias\n",
    "        \"\"\"\n",
    "        h = self.backbone(Z)\n",
    "        out = self.head(h)  # [B, p_max+1]\n",
    "        \n",
    "        coeffs = out[:, :self.p_max]  # [B, p_max]\n",
    "        bias = out[:, self.p_max:]  # [B, 1]\n",
    "        \n",
    "        # Dot product + bias\n",
    "        pred = (coeffs * Z).sum(dim=1, keepdim=True) + bias\n",
    "        \n",
    "        return pred.squeeze(1), coeffs, bias.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f5e4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Neural Operator (Continuous Delay Kernel)\n",
    "\n",
    "**Source**: `NeuralOperator.ipynb`\n",
    "\n",
    "**Key Idea**: Model TVAR as an integral operator over continuous delays:\n",
    "\n",
    "$$\\hat{x}_t = c(t) + \\int_{\\tau_{\\min}}^{\\tau_{\\max}} k_t(\\tau) \\, x(t-\\tau) \\, d\\tau$$\n",
    "\n",
    "The kernel $k_t(\\tau)$ is parameterized via Fourier features over $\\tau$:\n",
    "\n",
    "$$k_t(\\tau) = \\sum_{m=1}^{M} \\left[ a_m(t) \\cos(\\omega_m \\tau) + b_m(t) \\sin(\\omega_m \\tau) \\right]$$\n",
    "\n",
    "where the amplitudes $a_m(t), b_m(t)$ are produced by a causal context encoder.\n",
    "\n",
    "**Key Advantage**: Sampling-rate invariant — can handle non-uniform or varying $\\Delta t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Neural Operator — Continuous Delay Kernel (from NeuralOperator.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def fractional_delay_samples(x, tau_grid, dt, t_offset=0):\n",
    "    \"\"\"\n",
    "    Sample x(t - τ) with linear interpolation for continuous delays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor, shape (B, T)\n",
    "        Input time series\n",
    "    tau_grid : Tensor, shape (L,)\n",
    "        Delay values in seconds\n",
    "    dt : float\n",
    "        Sampling interval in seconds\n",
    "    t_offset : int\n",
    "        Offset for absolute time indexing\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Xlags : Tensor, shape (B, T, L)\n",
    "        Interpolated lagged values\n",
    "    \"\"\"\n",
    "    B, T = x.shape\n",
    "    device, dtype = x.device, x.dtype\n",
    "\n",
    "    if isinstance(tau_grid, torch.Tensor):\n",
    "        tau_idx = tau_grid.to(device=device, dtype=dtype) / float(dt)\n",
    "    else:\n",
    "        tau_idx = torch.as_tensor(tau_grid, device=device, dtype=dtype) / float(dt)\n",
    "\n",
    "    t_idx = torch.arange(T, device=device, dtype=dtype).view(1, T, 1) + float(t_offset)\n",
    "\n",
    "    src = t_idx - tau_idx.view(1, 1, -1)  # [1, T, L]\n",
    "    src0 = torch.clamp(torch.floor(src), 0, T - 1).to(torch.long)\n",
    "    src1 = torch.clamp(src0 + 1, 0, T - 1)\n",
    "    w = (src - src0.to(dtype)).to(dtype)\n",
    "\n",
    "    idx0 = src0.expand(B, -1, -1).contiguous()\n",
    "    idx1 = src1.expand(B, -1, -1).contiguous()\n",
    "\n",
    "    L = tau_idx.numel()\n",
    "    x_exp = x.unsqueeze(-1).repeat(1, 1, L).contiguous()\n",
    "\n",
    "    x0 = torch.gather(x_exp, 1, idx0)\n",
    "    x1 = torch.gather(x_exp, 1, idx1)\n",
    "    \n",
    "    return (1 - w) * x0 + w * x1\n",
    "\n",
    "\n",
    "def total_variation_time(k):\n",
    "    \"\"\"Total variation regularizer over time dimension.\"\"\"\n",
    "    return (k[:, 1:, :] - k[:, :-1, :]).abs().mean()\n",
    "\n",
    "\n",
    "def l1_energy(k):\n",
    "    \"\"\"L1 sparsity regularizer.\"\"\"\n",
    "    return k.abs().mean()\n",
    "\n",
    "\n",
    "class TVAROperator(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-Varying AR as a Neural Operator over continuous delays.\n",
    "    \n",
    "    Models the prediction as an integral:\n",
    "        y_t = c(t) + ∫ k_t(τ) x(t-τ) dτ\n",
    "    \n",
    "    The kernel k_t(τ) is parameterized with Fourier features over τ,\n",
    "    with time-varying amplitudes from a causal context encoder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        Number of delay points to discretize\n",
    "    tau_min : float\n",
    "        Minimum delay (seconds)\n",
    "    tau_max : float\n",
    "        Maximum delay (seconds)\n",
    "    n_modes : int\n",
    "        Number of Fourier modes for kernel\n",
    "    hidden : int\n",
    "        Channels in context encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, L=128, tau_min=0.0, tau_max=0.5, n_modes=16, hidden=64):\n",
    "        super().__init__()\n",
    "        assert tau_max > tau_min >= 0.0\n",
    "        self.L = L\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_max = tau_max\n",
    "        self.register_buffer(\"tau_grid\", torch.linspace(tau_min, tau_max, L))\n",
    "\n",
    "        # Causal context encoder (1D convs, left-padded)\n",
    "        self.ctx = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden, kernel_size=9, padding=8, dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel_size=5, padding=4, dilation=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fourier basis over τ\n",
    "        self.register_buffer(\"freqs\", torch.linspace(0.0, math.pi, n_modes))\n",
    "        self.head_a = nn.Linear(hidden, n_modes)  # cos amplitudes\n",
    "        self.head_b = nn.Linear(hidden, n_modes)  # sin amplitudes\n",
    "        self.bias = nn.Linear(hidden, 1)  # c(t), time-varying intercept\n",
    "\n",
    "        # Global gain for stability\n",
    "        self.kernel_gain = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def make_kernel(self, h):\n",
    "        \"\"\"\n",
    "        Generate time-varying kernel from context features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        h : Tensor, shape (B, T, H)\n",
    "            Context features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        k : Tensor, shape (B, T, L)\n",
    "            Kernel values over delay grid\n",
    "        \"\"\"\n",
    "        B, T, H = h.shape\n",
    "        a = self.head_a(h)  # [B, T, M]\n",
    "        b = self.head_b(h)  # [B, T, M]\n",
    "        \n",
    "        # Fourier features over τ\n",
    "        tau = self.tau_grid.view(1, 1, self.L, 1)  # [1, 1, L, 1]\n",
    "        omega = self.freqs.view(1, 1, 1, -1)  # [1, 1, 1, M]\n",
    "        cosF = torch.cos(omega * tau)  # [1, 1, L, M]\n",
    "        sinF = torch.sin(omega * tau)  # [1, 1, L, M]\n",
    "        \n",
    "        # Combine with time-varying amplitudes\n",
    "        k = (a.unsqueeze(2) * cosF + b.unsqueeze(2) * sinF).sum(-1)  # [B, T, L]\n",
    "        \n",
    "        return self.kernel_gain * k\n",
    "\n",
    "    def forward(self, x, dt, t_offset=0, return_kernel=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (B, T)\n",
    "            Input time series\n",
    "        dt : float\n",
    "            Sampling interval in seconds\n",
    "        t_offset : int\n",
    "            Offset for absolute time indexing\n",
    "        return_kernel : bool\n",
    "            If True, also return intermediate values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        yhat : Tensor, shape (B, T)\n",
    "            Predicted values\n",
    "        k : Tensor, shape (B, T, L)\n",
    "            Kernel (if return_kernel=True)\n",
    "        c : Tensor, shape (B, T)\n",
    "            Bias (if return_kernel=True)\n",
    "        Xlags : Tensor, shape (B, T, L)\n",
    "            Lagged values (if return_kernel=True)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "\n",
    "        # Causal context features\n",
    "        x1 = x.unsqueeze(1)  # [B, 1, T]\n",
    "        h = self.ctx(F.pad(x1, (32, 0)))  # [B, H, T+32]\n",
    "        h = h[..., -T:]  # Crop to length T\n",
    "        hT = h.transpose(1, 2)  # [B, T, H]\n",
    "\n",
    "        k = self.make_kernel(hT)  # [B, T, L]\n",
    "        c = self.bias(hT).squeeze(-1)  # [B, T]\n",
    "\n",
    "        # Sample lagged signal at continuous τ-grid\n",
    "        Xlags = fractional_delay_samples(x, self.tau_grid, float(dt), t_offset=t_offset)\n",
    "\n",
    "        # Riemann sum approximation of integral\n",
    "        delta_tau = (self.tau_max - self.tau_min) / max(self.L - 1, 1)\n",
    "        yhat = (k * Xlags).sum(-1) * delta_tau + c  # [B, T]\n",
    "\n",
    "        if return_kernel:\n",
    "            return yhat, k, c, Xlags\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d4bc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Utilities\n",
    "\n",
    "Common utilities for data preparation and training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1313c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. AR(p) Baseline (OLS)\n",
    "\n",
    "**Key Idea**: Classic autoregressive model with fixed coefficients, estimated via Ordinary Least Squares.\n",
    "\n",
    "$$\\hat{x}_t = \\sum_{i=1}^{p} a_i \\, x_{t-i} + \\epsilon_t$$\n",
    "\n",
    "The coefficients $\\mathbf{a} = [a_1, \\ldots, a_p]$ are estimated by minimizing:\n",
    "\n",
    "$$\\min_{\\mathbf{a}} \\| \\mathbf{y} - \\mathbf{X} \\mathbf{a} \\|^2$$\n",
    "\n",
    "**Use Case**: Baseline for stationary signals. Compare TVAR models against this to verify time-varying behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AR(p) Baseline — OLS Estimation\n",
    "# ==============================================================================\n",
    "\n",
    "class ARModel:\n",
    "    \"\"\"\n",
    "    Classic AR(p) model with OLS estimation.\n",
    "    \n",
    "    A simple baseline for comparison with time-varying models.\n",
    "    Uses numpy for fitting (no PyTorch needed).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : int\n",
    "        AR order (number of lags)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    coeffs : ndarray, shape (p,)\n",
    "        Estimated AR coefficients [a_1, ..., a_p]\n",
    "    intercept : float\n",
    "        Estimated intercept term\n",
    "    residual_std : float\n",
    "        Standard deviation of residuals\n",
    "    \"\"\"\n",
    "    def __init__(self, p: int):\n",
    "        self.p = p\n",
    "        self.coeffs = None\n",
    "        self.intercept = None\n",
    "        self.residual_std = None\n",
    "    \n",
    "    def fit(self, x: np.ndarray) -> \"ARModel\":\n",
    "        \"\"\"\n",
    "        Fit AR(p) model using OLS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray, shape (T,)\n",
    "            Time series to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : ARModel\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        T = len(x)\n",
    "        p = self.p\n",
    "        \n",
    "        # Build design matrix X and target y\n",
    "        X = np.column_stack([x[p-k:T-k] for k in range(1, p+1)])  # (T-p, p)\n",
    "        y = x[p:]  # (T-p,)\n",
    "        \n",
    "        # Add intercept column\n",
    "        X_with_intercept = np.column_stack([np.ones(len(y)), X])  # (T-p, p+1)\n",
    "        \n",
    "        # OLS: β = (X'X)^{-1} X'y\n",
    "        beta = np.linalg.lstsq(X_with_intercept, y, rcond=None)[0]\n",
    "        \n",
    "        self.intercept = beta[0]\n",
    "        self.coeffs = beta[1:]\n",
    "        \n",
    "        # Compute residuals\n",
    "        y_hat = X_with_intercept @ beta\n",
    "        residuals = y - y_hat\n",
    "        self.residual_std = np.std(residuals)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        One-step ahead prediction (teacher-forced).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray, shape (T,)\n",
    "            Input time series\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : ndarray, shape (T-p,)\n",
    "            Predictions for x[p], x[p+1], ...\n",
    "        \"\"\"\n",
    "        if self.coeffs is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        T = len(x)\n",
    "        p = self.p\n",
    "        \n",
    "        X = np.column_stack([x[p-k:T-k] for k in range(1, p+1)])\n",
    "        y_hat = self.intercept + X @ self.coeffs\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def forecast(self, x: np.ndarray, horizon: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Multi-step ahead forecast (autoregressive rollout).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray, shape (T,)\n",
    "            Input time series (uses last p values as initial condition)\n",
    "        horizon : int\n",
    "            Number of steps to forecast\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        forecast : ndarray, shape (horizon,)\n",
    "            Forecasted values\n",
    "        \"\"\"\n",
    "        if self.coeffs is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        p = self.p\n",
    "        buffer = list(x[-p:])  # Last p values\n",
    "        forecast = []\n",
    "        \n",
    "        for _ in range(horizon):\n",
    "            lags = np.array(buffer[-p:][::-1])  # [x_{t-1}, ..., x_{t-p}]\n",
    "            y_next = self.intercept + np.dot(self.coeffs, lags)\n",
    "            forecast.append(y_next)\n",
    "            buffer.append(y_next)\n",
    "        \n",
    "        return np.array(forecast)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.coeffs is None:\n",
    "            return f\"ARModel(p={self.p}, fitted=False)\"\n",
    "        return f\"ARModel(p={self.p}, coeffs={self.coeffs.round(4)}, intercept={self.intercept:.4f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Common Utilities\n",
    "# ==============================================================================\n",
    "\n",
    "def make_lags(x, p):\n",
    "    \"\"\"\n",
    "    Create lagged design matrix for AR(p) regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (T,)\n",
    "        Time series\n",
    "    p : int\n",
    "        Lag order\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray, shape (T-p, p)\n",
    "        Lagged features [x_{t-1}, ..., x_{t-p}]\n",
    "    y : ndarray, shape (T-p,)\n",
    "        Target values x_t\n",
    "    \"\"\"\n",
    "    X = np.stack([x[p-k:len(x)-k] for k in range(1, p+1)], axis=1)\n",
    "    y = x[p:]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_phi_seq(x, p=2):\n",
    "    \"\"\"\n",
    "    Create lagged feature sequence for Neural ODE TVAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (T,)\n",
    "        Time series\n",
    "    p : int\n",
    "        Number of lags\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    phi : ndarray, shape (T, p)\n",
    "        Lagged features (zero-padded for t < p)\n",
    "    \"\"\"\n",
    "    phi = np.stack([np.roll(x, k) for k in range(1, p+1)], axis=1).astype(np.float32)\n",
    "    phi[:p] = 0.0\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc809620",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model | Key Idea | Stability | Continuous τ | Time-Varying | Source |\n",
    "|-------|----------|-----------|--------------|--------------|--------|\n",
    "| **ARModel** | OLS baseline | ✅ (check roots) | ❌ | ❌ Fixed | — |\n",
    "| **NeuralODE_TVAR2** | ODE evolves latent → Levinson-Durbin | ✅ Guaranteed | ❌ | ✅ | `NeuralODE.ipynb` |\n",
    "| **LagAttentionTVAR** | Transformer attention over lag bank | ❌ Learned | ❌ | ✅ | `DeepLagAttention.ipynb` |\n",
    "| **LagAttentionTVARFast** | Bilinear scoring (no transformer) | ❌ Learned | ❌ | ✅ | `DeepLagAttention.ipynb` |\n",
    "| **TransformerAR** | CLS token over lag sequence | ❌ N/A | ❌ | ❌ Nonlinear | `fixed_linear_transformer.ipynb` |\n",
    "| **MLPTVAR** | Hyper-network outputs coefficients | ❌ Learned | ❌ | ✅ | `tvar_linear_mlp.ipynb` |\n",
    "| **TVAROperator** | Fourier kernel over continuous delays | ❌ Regularized | ✅ | ✅ | `NeuralOperator.ipynb` |\n",
    "\n",
    "### Next Steps\n",
    "- Move models to `stochastic_dynamics/models/`\n",
    "- Add training loops and evaluation metrics\n",
    "- Benchmark on simulated TVAR data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
