{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37920784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f51bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tvar_neural_operator.py\n",
    "# # Time-Varying AR as a Neural Operator over Continuous Delays\n",
    "# # - Learns k_t(τ) with Fourier features (operator-style kernel in delay domain)\n",
    "# # - Works across sampling rates via continuous-τ interpolation\n",
    "# # - Includes multi-horizon loss and refresh-every-k rollout\n",
    "\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # ----------------------------\n",
    "# # Utilities\n",
    "# # ----------------------------\n",
    "# def to_tensor(x, device=\"cpu\", dtype=torch.float32):\n",
    "#     if isinstance(x, np.ndarray):\n",
    "#         x = torch.from_numpy(x)\n",
    "#     return x.to(device=device, dtype=dtype)\n",
    "\n",
    "# def fractional_delay_samples(x,  # [B, T]\n",
    "#                              tau_grid,  # [L] delays in *seconds*\n",
    "#                              dt,        # scalar seconds per sample\n",
    "#                              t_offset=0):\n",
    "#     \"\"\"\n",
    "#     Gather x(t - τ_ℓ) with linear interpolation for a batch over time.\n",
    "#     - x: [B,T]\n",
    "#     - tau_grid: [L] (seconds), monotone increasing (e.g., np.linspace(Δmin, Δmax, L))\n",
    "#     - dt: float (seconds/sample)\n",
    "#     - t_offset: starting absolute time index (in samples) for the first output time step.\n",
    "#       For plain teacher-forced 1-step at times t = 0..T-1, set t_offset=0.\n",
    "#       For test windows taken from the middle of a long series, set t_offset to that start index.\n",
    "#     Returns:\n",
    "#       Xlags: [B, T, L] with x(t - τℓ).\n",
    "#     \"\"\"\n",
    "#     B, T = x.shape\n",
    "#     device = x.device\n",
    "#     L = len(tau_grid)\n",
    "#     # Convert τ (sec) -> τ_idx (samples)\n",
    "#     tau_idx = to_tensor(np.asarray(tau_grid) / dt, device=device)  # [L]\n",
    "#     # times in *sample indices* for each step we predict\n",
    "#     t_idx = to_tensor(np.arange(T) + t_offset, device=device).view(1, T, 1)  # [1,T,1]\n",
    "#     # desired source indices (float)\n",
    "#     src = t_idx - tau_idx.view(1, 1, L)  # [1,T,L]\n",
    "#     src0 = torch.clamp(torch.floor(src), 0, T - 1)        # lower index\n",
    "#     src1 = torch.clamp(src0 + 1, 0, T - 1)                # upper index\n",
    "#     w = (src - src0).to(x.dtype)                          # interpolation weight\n",
    "\n",
    "#     # Gather with batch broadcasting\n",
    "#     idx0 = src0.long().expand(B, -1, -1)  # [B,T,L]\n",
    "#     idx1 = src1.long().expand(B, -1, -1)  # [B,T,L]\n",
    "#     x_exp = x.unsqueeze(-1).expand(-1, -1, L)  # [B,T,L] (for fancy indexing we gather per time below)\n",
    "#     # We need to gather per time step; easiest is to reshape\n",
    "#     x0 = torch.gather(x_exp, 1, idx0)  # [B,T,L]\n",
    "#     x1 = torch.gather(x_exp, 1, idx1)  # [B,T,L]\n",
    "#     return (1 - w) * x0 + w * x1       # [B,T,L]\n",
    "\n",
    "# def total_variation_time(k):\n",
    "#     # k: [B,T,L] -> TV along time\n",
    "#     return (k[:, 1:, :] - k[:, :-1, :]).abs().mean()\n",
    "\n",
    "# def l1_energy(k):\n",
    "#     return k.abs().mean()\n",
    "\n",
    "# # ----------------------------\n",
    "# # Model: TVAROperator\n",
    "# # ----------------------------\n",
    "# class TVAROperator(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Time-varying AR as a neural operator over continuous delays:\n",
    "#       y_t = c(t) + ∫ k_t(τ) x(t-τ) dτ  ≈  c(t) + Σ k_t(τ_ℓ) x(t-τ_ℓ) Δτ\n",
    "#     We discretize τ on a grid and parameterize k_t(τ) via Fourier features of τ,\n",
    "#     with time-varying amplitudes produced by a small causal context encoder.\n",
    "\n",
    "#     Inputs:\n",
    "#       L        : number of delay points (τ samples) within [tau_min, tau_max]\n",
    "#       tau_min  : minimum delay (seconds) > 0\n",
    "#       tau_max  : maximum delay (seconds) > tau_min\n",
    "#       n_modes  : # Fourier modes for kernel over τ\n",
    "#       hidden   : channels in context encoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, L=128, tau_min=0.0, tau_max=0.5, n_modes=16, hidden=64):\n",
    "#         super().__init__()\n",
    "#         assert tau_max > tau_min >= 0.0\n",
    "#         self.L = L\n",
    "#         self.tau_min = tau_min\n",
    "#         self.tau_max = tau_max\n",
    "#         self.register_buffer(\"tau_grid\", torch.linspace(tau_min, tau_max, L))  # seconds\n",
    "\n",
    "#         # Causal context encoder over time (1D convs, left padding)\n",
    "#         self.ctx = nn.Sequential(\n",
    "#             nn.Conv1d(1, hidden, kernel_size=9, padding=8, dilation=2), nn.ReLU(),\n",
    "#             nn.Conv1d(hidden, hidden, kernel_size=5, padding=4, dilation=2), nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Fourier basis over τ (fixed frequencies 0..π * modes)\n",
    "#         self.register_buffer(\"freqs\", torch.linspace(0.0, math.pi, n_modes))\n",
    "#         self.head_a = nn.Linear(hidden, n_modes)  # cos amplitudes\n",
    "#         self.head_b = nn.Linear(hidden, n_modes)  # sin amplitudes\n",
    "#         self.bias   = nn.Linear(hidden, 1)        # c(t), time-varying intercept\n",
    "\n",
    "#         # Optional global gain to stabilize scale\n",
    "#         self.kernel_gain = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "#     def make_kernel(self, h):\n",
    "#         \"\"\"\n",
    "#         h: [B,T,H] context features -> k: [B,T,L] kernel over delays τ\n",
    "#         k_t(τ) = Σ_m [ a_m(t) cos(ω_m τ) + b_m(t) sin(ω_m τ) ]\n",
    "#         \"\"\"\n",
    "#         B, T, H = h.shape\n",
    "#         a = self.head_a(h)  # [B,T,M]\n",
    "#         b = self.head_b(h)  # [B,T,M]\n",
    "#         # Build Fourier features over τ once, broadcast to batch/time\n",
    "#         tau = self.tau_grid.view(1, 1, self.L, 1)         # [1,1,L,1]\n",
    "#         omega = self.freqs.view(1, 1, 1, -1)              # [1,1,1,M]\n",
    "#         cosF = torch.cos(omega * tau)                     # [1,1,L,M]\n",
    "#         sinF = torch.sin(omega * tau)                     # [1,1,L,M]\n",
    "#         # combine with amplitudes\n",
    "#         k = (a.unsqueeze(2) * cosF + b.unsqueeze(2) * sinF).sum(-1)  # [B,T,L]\n",
    "#         return self.kernel_gain * k\n",
    "\n",
    "#     # def forward(self, x, dt, t_offset=0, return_kernel=False):\n",
    "#     #     \"\"\"\n",
    "#     #     x: [B,T] observed scalar series\n",
    "#     #     dt: float seconds per sample (tensor or python float)\n",
    "#     #     t_offset: absolute index (samples) for time=0 of x wrt the original series (for eval alignment)\n",
    "#     #     returns:\n",
    "#     #       yhat: [B,T]\n",
    "#     #       (optional) k: [B,T,L], c: [B,T], Xlags: [B,T,L]\n",
    "#     #     \"\"\"\n",
    "#     #     B, T = x.shape\n",
    "#     #     # causal context features\n",
    "#     #     # x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "#     #     # h  = self.ctx(F.pad(x1, (32, 0)))                 # left pad -> causal\n",
    "#     #     # hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "#     #     # k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "#     #     # c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "\n",
    "#     #     # # sample lagged signal at continuous τ-grid\n",
    "#     #     # Xlags = fractional_delay_samples(x, self.tau_grid, float(dt), t_offset=t_offset)  # [B,T,L]\n",
    "\n",
    "#     #    # inside TVAROperator.forward(...)\n",
    "#     #     x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "#     #     h  = self.ctx(F.pad(x1, (32, 0)))                 # [B,H,T+32] due to extra pad\n",
    "#     #     h  = h[..., -x.shape[-1]:]                        # <-- crop to last T to align\n",
    "#     #     hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "#     #     k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "#     #     c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "#     #     Xlags = fractional_delay_samples(x, self.tau_grid, float(dt), t_offset=t_offset)  # [B,T,L]\n",
    "#     #     yhat = (k * Xlags).sum(-1) * delta_tau + c\n",
    "\n",
    "#     #     # Riemann sum over τ (Δτ constant)\n",
    "#     #     delta_tau = (self.tau_max - self.tau_min) / max(self.L - 1, 1)\n",
    "#     #     yhat = (k * Xlags).sum(-1) * delta_tau + c        # [B,T]\n",
    "#     #     if return_kernel:\n",
    "#     #         return yhat, k, c, Xlags\n",
    "#     #     return yhat\n",
    "#     def forward(self, x, dt, t_offset=0, return_kernel=False):\n",
    "#         B, T = x.shape\n",
    "\n",
    "#         # causal context features\n",
    "#         x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "#         h  = self.ctx(F.pad(x1, (32, 0)))                 # [B,H,T+32] due to extra left pad\n",
    "#         h  = h[..., -T:]                                   # <-- crop back to length T\n",
    "#         hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "\n",
    "#         k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "#         c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "\n",
    "#         # sample lagged signal at continuous τ-grid\n",
    "#         Xlags = fractional_delay_samples(\n",
    "#             x, self.tau_grid, float(dt), t_offset=t_offset\n",
    "#         )                                                 # [B,T,L]\n",
    "\n",
    "#         # Riemann sum over τ (Δτ constant)  <-- define BEFORE yhat\n",
    "#         delta_tau = (self.tau_max - self.tau_min) / max(self.L - 1, 1)\n",
    "\n",
    "#         yhat = (k * Xlags).sum(-1) * delta_tau + c        # [B,T]\n",
    "\n",
    "#         if return_kernel:\n",
    "#             return yhat, k, c, Xlags\n",
    "#         return yhat\n",
    "\n",
    "\n",
    "# # ----------------------------\n",
    "# # Losses & training helpers\n",
    "# # ----------------------------\n",
    "# def rollout_multi_horizon(model, x, dt, horizons, refresh_every=1):\n",
    "#     \"\"\"\n",
    "#     Multi-horizon prediction with optional refresh-every-k hybrid.\n",
    "#     - x: [B,T] full sequence used for teacher forcing at refresh points\n",
    "#     - horizons: list like [1,5,20]\n",
    "#     Returns:\n",
    "#       dict: {h: yhat_h [B,T]} where yhat_h aligns so that yhat_h[:, t] predicts x[:, t+h]\n",
    "#     \"\"\"\n",
    "#     B, T = x.shape\n",
    "#     device = x.device\n",
    "#     outs = {}\n",
    "#     # For each horizon, we do a causal pass that mixes teacher-forced prefixes and open-loop segments.\n",
    "#     # Simple approach: simulate step-by-step and store predictions for all horizons we need.\n",
    "#     max_h = max(horizons)\n",
    "#     # buffer for open-loop generation\n",
    "#     x_gen = x.clone()  # start from truth; we overwrite future points when we go open-loop\n",
    "\n",
    "#     for t in range(T - max_h):\n",
    "#         # every refresh_every steps, reset generator segment to truth\n",
    "#         if (t % max(1, int(refresh_every))) == 0:\n",
    "#             # ensure the last window aligns with truth up to t\n",
    "#             x_gen[:, :t+1] = x[:, :t+1]\n",
    "\n",
    "#         # predict next step using the operator at the *current* dt\n",
    "#         # we need a window covering (at least) the τ_max back in time; the model samples internally\n",
    "#         # For efficiency, we call model once per block; here we do a tiny call that returns the immediate next step.\n",
    "#         yhat_step = model(x_gen[:, :t+1], dt, t_offset=0)  # [B, t+1]\n",
    "#         next_pred = yhat_step[:, -1]                       # [B]\n",
    "#         # write predicted (open-loop) step into x_gen\n",
    "#         x_gen[:, t+1] = next_pred\n",
    "\n",
    "#         # record the horizons that equal 1 at this step (and later ones below)\n",
    "#         # We'll collect full arrays after the loop.\n",
    "#         pass\n",
    "\n",
    "#     # After generating, compute horizon-specific aligned predictions in one shot\n",
    "#     for h in horizons:\n",
    "#         # For each t, yhat_h[t] should be the model's prediction of x[t+h].\n",
    "#         # A simple approximation from the above loop is to take x_gen shifted by -h.\n",
    "#         yhat_h = torch.zeros_like(x)\n",
    "#         yhat_h[:, :-h] = x_gen[:, h:]\n",
    "#         yhat_h[:, -h:] = x_gen[:, -1:].expand(-1, h)  # dummy fill (unused in loss)\n",
    "#         outs[h] = yhat_h\n",
    "#     return outs\n",
    "\n",
    "# def loss_step(model, batch_x, dt, horizons=(1,5,20), refresh_every=1,\n",
    "#               lambda_tv=1e-3, lambda_l1=1e-4):\n",
    "#     \"\"\"\n",
    "#     Compute:\n",
    "#       - 1-step MSE (teacher-forced, direct forward)\n",
    "#       - multi-horizon rollout MSE with refresh-every-k hybrid\n",
    "#       - TV and L1 regularization on kernels\n",
    "#     \"\"\"\n",
    "#     y1, k, c, _ = model(batch_x, dt, return_kernel=True)   # teacher-forced pass\n",
    "#     # 1-step alignment: y1[:, :-1] predicts x[:, 1:]\n",
    "#     mse_1 = F.mse_loss(y1[:, :-1], batch_x[:, 1:])\n",
    "\n",
    "#     # rollout losses\n",
    "#     outs = rollout_multi_horizon(model, batch_x, dt, horizons, refresh_every=refresh_every)\n",
    "#     mse_roll = 0.0\n",
    "#     for h in horizons:\n",
    "#         # only compare where target exists\n",
    "#         mse_roll = mse_roll + F.mse_loss(outs[h][:, :-h], batch_x[:, h:])\n",
    "#     mse_roll = mse_roll / len(horizons)\n",
    "\n",
    "#     # regularization\n",
    "#     reg = lambda_tv * total_variation_time(k) + lambda_l1 * l1_energy(k)\n",
    "#     return mse_1, mse_roll, reg, {\"k\": k.detach()}\n",
    "\n",
    "# # ----------------------------\n",
    "# # Data prep: toy Lorenz x(t)\n",
    "# # ----------------------------\n",
    "# def lorenz(T_steps=30000, dt=0.005, sigma=10.0, rho=28.0, beta=8/3, x0=(1.0,1.0,1.0)):\n",
    "#     x, y, z = x0\n",
    "#     xs, ys, zs = [], [], []\n",
    "#     for _ in range(T_steps):\n",
    "#         def f(x,y,z):\n",
    "#             dx = sigma*(y-x)\n",
    "#             dy = x*(rho - z) - y\n",
    "#             dz = x*y - beta*z\n",
    "#             return dx, dy, dz\n",
    "#         k1 = f(x,y,z)\n",
    "#         k2 = f(x + 0.5*dt*k1[0], y + 0.5*dt*k1[1], z + 0.5*dt*k1[2])\n",
    "#         k3 = f(x + 0.5*dt*k2[0], y + 0.5*dt*k2[1], z + 0.5*dt*k2[2])\n",
    "#         k4 = f(x + dt*k3[0], y + dt*k3[1], z + dt*k3[2])\n",
    "#         x += (dt/6.0)*(k1[0] + 2*k2[0] + 2*k3[0] + k4[0])\n",
    "#         y += (dt/6.0)*(k1[1] + 2*k2[1] + 2*k3[1] + k4[1])\n",
    "#         z += (dt/6.0)*(k1[2] + 2*k2[2] + 2*k3[2] + k4[2])\n",
    "#         xs.append(x); ys.append(y); zs.append(z)\n",
    "#     return np.array(xs), np.array(ys), np.array(zs)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Minimal training loop (demo)\n",
    "# # ----------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#     # --- Generate toy data (replace with your real series) ---\n",
    "#     dt0 = 0.005\n",
    "#     xs, ys, zs = lorenz(T_steps=20000, dt=dt0)\n",
    "#     x_np = xs.astype(np.float32)\n",
    "#     # standardize (optional but helpful)\n",
    "#     x_np = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "\n",
    "#     # Train / test split\n",
    "#     T = len(x_np)\n",
    "#     split = int(0.8 * T)\n",
    "#     x_train = to_tensor(x_np[:split][None, :], device)   # [B=1, T_tr]\n",
    "#     x_test  = to_tensor(x_np[split:][None, :], device)   # [B=1, T_te]\n",
    "\n",
    "#     # --- Build model ---\n",
    "#     # Choose τ window in *seconds* (e.g., up to 0.5 s; tune to your data)\n",
    "#     tau_min, tau_max = 0.0, 0.5\n",
    "#     model = TVAROperator(L=128, tau_min=tau_min, tau_max=tau_max, n_modes=16, hidden=64).to(device)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-6)\n",
    "\n",
    "#     # --- Train ---\n",
    "#     horizons = (1, 5, 20)\n",
    "#     refresh_every = 5   # hybrid rollout during training (matches your pipeline idea)\n",
    "\n",
    "#     for epoch in range(20):\n",
    "#         model.train()\n",
    "#         mse1, mser, reg, extras = loss_step(model, x_train, dt0, horizons=horizons,\n",
    "#                                             refresh_every=refresh_every,\n",
    "#                                             lambda_tv=1e-3, lambda_l1=1e-4)\n",
    "#         loss = mse1 + mser + reg\n",
    "#         opt.zero_grad(set_to_none=True)\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         opt.step()\n",
    "\n",
    "#         if (epoch+1) % 5 == 0:\n",
    "#             print(f\"[{epoch+1:03d}] 1-step: {mse1.item():.4e}  roll:{mser.item():.4e}  reg:{reg.item():.4e}\")\n",
    "\n",
    "#     # --- Evaluate on test at original dt ---\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Teacher-forced 1-step MSE\n",
    "#         yhat_test = model(x_test, dt0)        # [1,Tte]\n",
    "#         mse1_te = F.mse_loss(yhat_test[:, :-1], x_test[:, 1:]).item()\n",
    "\n",
    "#         # Hybrid rollout with your knob (refresh-every-k)\n",
    "#         k_refresh = 20\n",
    "#         outs = rollout_multi_horizon(model, x_test, dt0, horizons=(1,5,20,50), refresh_every=k_refresh)\n",
    "#         mse_roll_20 = F.mse_loss(outs[20][:, :-20], x_test[:, 20:]).item()\n",
    "#         print(f\"Test 1-step MSE: {mse1_te:.4e}   20-step (refresh={k_refresh}) MSE: {mse_roll_20:.4e}\")\n",
    "\n",
    "#     # --- (Key demo) Generalize to a different sampling rate ---\n",
    "#     # Downsample by 2 -> dt' = 2*dt0 ; the *same model* is used, only dt changes.\n",
    "#     with torch.no_grad():\n",
    "#         x_coarse = x_np[::2]\n",
    "#         x_coarse = to_tensor(((x_coarse - x_coarse.mean()) / (x_coarse.std() + 1e-8))[None, :], device)\n",
    "#         dt1 = dt0 * 2.0\n",
    "#         yhat_coarse = model(x_coarse, dt1)  # uses same τ-grid in seconds, resampled via interpolation\n",
    "#         mse1_coarse = F.mse_loss(yhat_coarse[:, :-1], x_coarse[:, 1:]).item()\n",
    "#         print(f\"Generalization to dt'={dt1:.4f}: 1-step MSE={mse1_coarse:.4e}\")\n",
    "\n",
    "#     # --- Inspect learned kernel on test (optional) ---\n",
    "#     with torch.no_grad():\n",
    "#         _, k_test, _, _ = model(x_test, dt0, return_kernel=True)  # [1,Tte,L]\n",
    "#         k_mean = k_test.mean(dim=1).squeeze(0).cpu().numpy()      # [L]\n",
    "#         print(\"Kernel summary over τ (mean over time): mean=\", k_mean.mean(), \" std=\", k_mean.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170ddc59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 376\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m    375\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 376\u001b[0m     mse1, mser, reg, extras \u001b[38;5;241m=\u001b[39m \u001b[43mloss_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlambda_tv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse1 \u001b[38;5;241m+\u001b[39m mser \u001b[38;5;241m+\u001b[39m reg\n\u001b[1;32m    380\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[11], line 312\u001b[0m, in \u001b[0;36mloss_step\u001b[0;34m(model, batch_x, dt, horizons, refresh_every, lambda_tv, lambda_l1)\u001b[0m\n\u001b[1;32m    309\u001b[0m mse_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y1[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], batch_x[:, \u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# rollout losses\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_multi_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m mse_roll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m horizons:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# only compare where target exists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 280\u001b[0m, in \u001b[0;36mrollout_multi_horizon\u001b[0;34m(model, x, dt, horizons, refresh_every)\u001b[0m\n\u001b[1;32m    275\u001b[0m     x_gen[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# predict next step using the operator at the *current* dt\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# we need a window covering (at least) the τ_max back in time; the model samples internally\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# For efficiency, we call model once per block; here we do a tiny call that returns the immediate next step.\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m yhat_step \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gen\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, t+1]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m next_pred \u001b[38;5;241m=\u001b[39m yhat_step[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]                       \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# write predicted (open-loop) step into x_gen\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 237\u001b[0m, in \u001b[0;36mTVAROperator.forward\u001b[0;34m(self, x, dt, t_offset, return_kernel)\u001b[0m\n\u001b[1;32m    234\u001b[0m c  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias(hT)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                    \u001b[38;5;66;03m# [B,T]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# sample lagged signal at continuous τ-grid\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m Xlags \u001b[38;5;241m=\u001b[39m \u001b[43mfractional_delay_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_offset\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m                                                 \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Riemann sum over τ (Δτ constant)  <-- define BEFORE yhat\u001b[39;00m\n\u001b[1;32m    242\u001b[0m delta_tau \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_min) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 109\u001b[0m, in \u001b[0;36mfractional_delay_samples\u001b[0;34m(x, tau_grid, dt, t_offset)\u001b[0m\n\u001b[1;32m    107\u001b[0m src0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(torch\u001b[38;5;241m.\u001b[39mfloor(src), \u001b[38;5;241m0\u001b[39m, T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)   \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m src1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(src0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,          \u001b[38;5;241m0\u001b[39m, T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)                 \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m w    \u001b[38;5;241m=\u001b[39m (src \u001b[38;5;241m-\u001b[39m \u001b[43msrc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(dtype)                         \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m idx0 \u001b[38;5;241m=\u001b[39m src0\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()                      \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m idx1 \u001b[38;5;241m=\u001b[39m src1\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()                      \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tvar_neural_operator.py\n",
    "# Time-Varying AR as a Neural Operator over Continuous Delays\n",
    "# - Learns k_t(τ) with Fourier features (operator-style kernel in delay domain)\n",
    "# - Works across sampling rates via continuous-τ interpolation\n",
    "# - Includes multi-horizon loss and refresh-every-k rollout\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def to_tensor(x, device=\"mpc\", dtype=torch.float32):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    return x.to(device=device, dtype=dtype)\n",
    "\n",
    "# def fractional_delay_samples(x,  # [B, T]\n",
    "#                              tau_grid,  # [L] delays in *seconds*\n",
    "#                              dt,        # scalar seconds per sample\n",
    "#                              t_offset=0):\n",
    "#     \"\"\"\n",
    "#     Gather x(t - τ_ℓ) with linear interpolation for a batch over time.\n",
    "#     - x: [B,T]\n",
    "#     - tau_grid: [L] (seconds), monotone increasing (e.g., np.linspace(Δmin, Δmax, L))\n",
    "#     - dt: float (seconds/sample)\n",
    "#     - t_offset: starting absolute time index (in samples) for the first output time step.\n",
    "#       For plain teacher-forced 1-step at times t = 0..T-1, set t_offset=0.\n",
    "#       For test windows taken from the middle of a long series, set t_offset to that start index.\n",
    "#     Returns:\n",
    "#       Xlags: [B, T, L] with x(t - τℓ).\n",
    "#     \"\"\"\n",
    "#     B, T = x.shape\n",
    "#     device = x.device\n",
    "#     L = len(tau_grid)\n",
    "#     # Convert τ (sec) -> τ_idx (samples)\n",
    "#     tau_idx = to_tensor(np.asarray(tau_grid) / dt, device=device)  # [L]\n",
    "#     # times in *sample indices* for each step we predict\n",
    "#     t_idx = to_tensor(np.arange(T) + t_offset, device=device).view(1, T, 1)  # [1,T,1]\n",
    "#     # desired source indices (float)\n",
    "#     src = t_idx - tau_idx.view(1, 1, L)  # [1,T,L]\n",
    "#     src0 = torch.clamp(torch.floor(src), 0, T - 1)        # lower index\n",
    "#     src1 = torch.clamp(src0 + 1, 0, T - 1)                # upper index\n",
    "#     w = (src - src0).to(x.dtype)                          # interpolation weight\n",
    "\n",
    "#     # Gather with batch broadcasting\n",
    "#     idx0 = src0.long().expand(B, -1, -1)  # [B,T,L]\n",
    "#     idx1 = src1.long().expand(B, -1, -1)  # [B,T,L]\n",
    "#     x_exp = x.unsqueeze(-1).expand(-1, -1, L)  # [B,T,L] (for fancy indexing we gather per time below)\n",
    "#     # We need to gather per time step; easiest is to reshape\n",
    "#     x0 = torch.gather(x_exp, 1, idx0)  # [B,T,L]\n",
    "#     x1 = torch.gather(x_exp, 1, idx1)  # [B,T,L]\n",
    "#     return (1 - w) * x0 + w * x1       # [B,T,L]\n",
    "\n",
    "# def fractional_delay_samples(x,  # [B, T]\n",
    "#                              tau_grid,  # [L] delays in *seconds* (torch tensor or numpy)\n",
    "#                              dt,        # float seconds per sample\n",
    "#                              t_offset=0):\n",
    "#     \"\"\"\n",
    "#     Gather x(t - τ_ℓ) with linear interpolation for a batch over time.\n",
    "#     Returns Xlags: [B, T, L]\n",
    "#     \"\"\"\n",
    "#     B, T = x.shape\n",
    "#     device = x.device\n",
    "#     dtype  = x.dtype\n",
    "\n",
    "#     # ---- make τ indices on the same device, no NumPy conversion ----\n",
    "#     if isinstance(tau_grid, torch.Tensor):\n",
    "#         tau_idx = tau_grid.to(device=device, dtype=dtype) / float(dt)   # [L]\n",
    "#     else:\n",
    "#         tau_idx = torch.as_tensor(tau_grid, device=device, dtype=dtype) / float(dt)  # [L]\n",
    "\n",
    "#     # time indices (in samples) for each step\n",
    "#     t_idx = torch.arange(T, device=device, dtype=dtype).view(1, T, 1) + float(t_offset)  # [1,T,1]\n",
    "\n",
    "#     # desired (float) source indices and interpolation weights\n",
    "#     src  = t_idx - tau_idx.view(1, 1, -1)                                  # [1,T,L]\n",
    "#     src0 = torch.clamp(torch.floor(src), 0, T - 1).to(torch.long)          # [1,T,L]\n",
    "#     src1 = torch.clamp(src0 + 1,          0, T - 1)                        # [1,T,L]\n",
    "#     w    = (src - src0.to(dtype)).to(dtype)                                 # [1,T,L]\n",
    "\n",
    "#     # gather values\n",
    "#     idx0 = src0.expand(B, -1, -1)                                          # [B,T,L]\n",
    "#     idx1 = src1.expand(B, -1, -1)                                          # [B,T,L]\n",
    "#     x_exp = x.unsqueeze(-1).expand(-1, -1, tau_idx.numel())                # [B,T,L]\n",
    "#     x0 = torch.gather(x_exp, 1, idx0)                                      # [B,T,L]\n",
    "#     x1 = torch.gather(x_exp, 1, idx1)                                      # [B,T,L]\n",
    "\n",
    "#     return (1 - w) * x0 + w * x1\n",
    "\n",
    "def fractional_delay_samples(x, tau_grid, dt, t_offset=0):\n",
    "    B, T = x.shape\n",
    "    device, dtype = x.device, x.dtype\n",
    "\n",
    "    # keep everything on-device; no numpy\n",
    "    if isinstance(tau_grid, torch.Tensor):\n",
    "        tau_idx = tau_grid.to(device=device, dtype=dtype) / float(dt)\n",
    "    else:\n",
    "        tau_idx = torch.as_tensor(tau_grid, device=device, dtype=dtype) / float(dt)\n",
    "\n",
    "    t_idx = torch.arange(T, device=device, dtype=dtype).view(1, T, 1) + float(t_offset)\n",
    "\n",
    "    src  = t_idx - tau_idx.view(1, 1, -1)                          # [1,T,L]\n",
    "    src0 = torch.clamp(torch.floor(src), 0, T - 1).to(torch.long)   # [1,T,L]\n",
    "    src1 = torch.clamp(src0 + 1,          0, T - 1)                 # [1,T,L]\n",
    "    w    = (src - src0.to(dtype)).to(dtype)                         # [1,T,L]\n",
    "\n",
    "    idx0 = src0.expand(B, -1, -1).contiguous()                      # [B,T,L]\n",
    "    idx1 = src1.expand(B, -1, -1).contiguous()                      # [B,T,L]\n",
    "\n",
    "    # use repeat (real memory) not expand (as_strided view)\n",
    "    L = tau_idx.numel()\n",
    "    x_exp = x.unsqueeze(-1).repeat(1, 1, L).contiguous()            # [B,T,L]\n",
    "\n",
    "    x0 = torch.gather(x_exp, 1, idx0)\n",
    "    x1 = torch.gather(x_exp, 1, idx1)\n",
    "    return (1 - w) * x0 + w * x1\n",
    "\n",
    "\n",
    "\n",
    "def total_variation_time(k):\n",
    "    # k: [B,T,L] -> TV along time\n",
    "    return (k[:, 1:, :] - k[:, :-1, :]).abs().mean()\n",
    "\n",
    "def l1_energy(k):\n",
    "    return k.abs().mean()\n",
    "\n",
    "# ----------------------------\n",
    "# Model: TVAROperator\n",
    "# ----------------------------\n",
    "class TVAROperator(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-varying AR as a neural operator over continuous delays:\n",
    "      y_t = c(t) + ∫ k_t(τ) x(t-τ) dτ  ≈  c(t) + Σ k_t(τ_ℓ) x(t-τ_ℓ) Δτ\n",
    "    We discretize τ on a grid and parameterize k_t(τ) via Fourier features of τ,\n",
    "    with time-varying amplitudes produced by a small causal context encoder.\n",
    "\n",
    "    Inputs:\n",
    "      L        : number of delay points (τ samples) within [tau_min, tau_max]\n",
    "      tau_min  : minimum delay (seconds) > 0\n",
    "      tau_max  : maximum delay (seconds) > tau_min\n",
    "      n_modes  : # Fourier modes for kernel over τ\n",
    "      hidden   : channels in context encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, L=128, tau_min=0.0, tau_max=0.5, n_modes=16, hidden=64):\n",
    "        super().__init__()\n",
    "        assert tau_max > tau_min >= 0.0\n",
    "        self.L = L\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_max = tau_max\n",
    "        self.register_buffer(\"tau_grid\", torch.linspace(tau_min, tau_max, L))  # seconds\n",
    "\n",
    "        # Causal context encoder over time (1D convs, left padding)\n",
    "        self.ctx = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden, kernel_size=9, padding=8, dilation=2), nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel_size=5, padding=4, dilation=2), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fourier basis over τ (fixed frequencies 0..π * modes)\n",
    "        self.register_buffer(\"freqs\", torch.linspace(0.0, math.pi, n_modes))\n",
    "        self.head_a = nn.Linear(hidden, n_modes)  # cos amplitudes\n",
    "        self.head_b = nn.Linear(hidden, n_modes)  # sin amplitudes\n",
    "        self.bias   = nn.Linear(hidden, 1)        # c(t), time-varying intercept\n",
    "\n",
    "        # Optional global gain to stabilize scale\n",
    "        self.kernel_gain = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def make_kernel(self, h):\n",
    "        \"\"\"\n",
    "        h: [B,T,H] context features -> k: [B,T,L] kernel over delays τ\n",
    "        k_t(τ) = Σ_m [ a_m(t) cos(ω_m τ) + b_m(t) sin(ω_m τ) ]\n",
    "        \"\"\"\n",
    "        B, T, H = h.shape\n",
    "        a = self.head_a(h)  # [B,T,M]\n",
    "        b = self.head_b(h)  # [B,T,M]\n",
    "        # Build Fourier features over τ once, broadcast to batch/time\n",
    "        tau = self.tau_grid.view(1, 1, self.L, 1)         # [1,1,L,1]\n",
    "        omega = self.freqs.view(1, 1, 1, -1)              # [1,1,1,M]\n",
    "        cosF = torch.cos(omega * tau)                     # [1,1,L,M]\n",
    "        sinF = torch.sin(omega * tau)                     # [1,1,L,M]\n",
    "        # combine with amplitudes\n",
    "        k = (a.unsqueeze(2) * cosF + b.unsqueeze(2) * sinF).sum(-1)  # [B,T,L]\n",
    "        return self.kernel_gain * k\n",
    "\n",
    "    # def forward(self, x, dt, t_offset=0, return_kernel=False):\n",
    "    #     \"\"\"\n",
    "    #     x: [B,T] observed scalar series\n",
    "    #     dt: float seconds per sample (tensor or python float)\n",
    "    #     t_offset: absolute index (samples) for time=0 of x wrt the original series (for eval alignment)\n",
    "    #     returns:\n",
    "    #       yhat: [B,T]\n",
    "    #       (optional) k: [B,T,L], c: [B,T], Xlags: [B,T,L]\n",
    "    #     \"\"\"\n",
    "    #     B, T = x.shape\n",
    "    #     # causal context features\n",
    "    #     # x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "    #     # h  = self.ctx(F.pad(x1, (32, 0)))                 # left pad -> causal\n",
    "    #     # hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "    #     # k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "    #     # c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "\n",
    "    #     # # sample lagged signal at continuous τ-grid\n",
    "    #     # Xlags = fractional_delay_samples(x, self.tau_grid, float(dt), t_offset=t_offset)  # [B,T,L]\n",
    "\n",
    "    #    # inside TVAROperator.forward(...)\n",
    "    #     x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "    #     h  = self.ctx(F.pad(x1, (32, 0)))                 # [B,H,T+32] due to extra pad\n",
    "    #     h  = h[..., -x.shape[-1]:]                        # <-- crop to last T to align\n",
    "    #     hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "    #     k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "    #     c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "    #     Xlags = fractional_delay_samples(x, self.tau_grid, float(dt), t_offset=t_offset)  # [B,T,L]\n",
    "    #     yhat = (k * Xlags).sum(-1) * delta_tau + c\n",
    "\n",
    "    #     # Riemann sum over τ (Δτ constant)\n",
    "    #     delta_tau = (self.tau_max - self.tau_min) / max(self.L - 1, 1)\n",
    "    #     yhat = (k * Xlags).sum(-1) * delta_tau + c        # [B,T]\n",
    "    #     if return_kernel:\n",
    "    #         return yhat, k, c, Xlags\n",
    "    #     return yhat\n",
    "    def forward(self, x, dt, t_offset=0, return_kernel=False):\n",
    "        B, T = x.shape\n",
    "\n",
    "        # causal context features\n",
    "        x1 = x.unsqueeze(1)                               # [B,1,T]\n",
    "        h  = self.ctx(F.pad(x1, (32, 0)))                 # [B,H,T+32] due to extra left pad\n",
    "        h  = h[..., -T:]                                   # <-- crop back to length T\n",
    "        hT = h.transpose(1, 2)                            # [B,T,H]\n",
    "\n",
    "        k  = self.make_kernel(hT)                         # [B,T,L]\n",
    "        c  = self.bias(hT).squeeze(-1)                    # [B,T]\n",
    "\n",
    "        # sample lagged signal at continuous τ-grid\n",
    "        Xlags = fractional_delay_samples(\n",
    "            x, self.tau_grid, float(dt), t_offset=t_offset\n",
    "        )                                                 # [B,T,L]\n",
    "\n",
    "        # Riemann sum over τ (Δτ constant)  <-- define BEFORE yhat\n",
    "        delta_tau = (self.tau_max - self.tau_min) / max(self.L - 1, 1)\n",
    "\n",
    "        yhat = (k * Xlags).sum(-1) * delta_tau + c        # [B,T]\n",
    "\n",
    "        if return_kernel:\n",
    "            return yhat, k, c, Xlags\n",
    "        return yhat\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Losses & training helpers\n",
    "# ----------------------------\n",
    "def rollout_multi_horizon(model, x, dt, horizons, refresh_every=1):\n",
    "    \"\"\"\n",
    "    Multi-horizon prediction with optional refresh-every-k hybrid.\n",
    "    - x: [B,T] full sequence used for teacher forcing at refresh points\n",
    "    - horizons: list like [1,5,20]\n",
    "    Returns:\n",
    "      dict: {h: yhat_h [B,T]} where yhat_h aligns so that yhat_h[:, t] predicts x[:, t+h]\n",
    "    \"\"\"\n",
    "    B, T = x.shape\n",
    "    device = x.device\n",
    "    outs = {}\n",
    "    # For each horizon, we do a causal pass that mixes teacher-forced prefixes and open-loop segments.\n",
    "    # Simple approach: simulate step-by-step and store predictions for all horizons we need.\n",
    "    max_h = max(horizons)\n",
    "    # buffer for open-loop generation\n",
    "    x_gen = x.clone()  # start from truth; we overwrite future points when we go open-loop\n",
    "\n",
    "    for t in range(T - max_h):\n",
    "        # every refresh_every steps, reset generator segment to truth\n",
    "        if (t % max(1, int(refresh_every))) == 0:\n",
    "            # ensure the last window aligns with truth up to t\n",
    "            x_gen[:, :t+1] = x[:, :t+1]\n",
    "\n",
    "        # predict next step using the operator at the *current* dt\n",
    "        # we need a window covering (at least) the τ_max back in time; the model samples internally\n",
    "        # For efficiency, we call model once per block; here we do a tiny call that returns the immediate next step.\n",
    "        yhat_step = model(x_gen[:, :t+1], dt, t_offset=0)  # [B, t+1]\n",
    "        next_pred = yhat_step[:, -1]                       # [B]\n",
    "        # write predicted (open-loop) step into x_gen\n",
    "        x_gen[:, t+1] = next_pred\n",
    "\n",
    "        # record the horizons that equal 1 at this step (and later ones below)\n",
    "        # We'll collect full arrays after the loop.\n",
    "        pass\n",
    "\n",
    "    # After generating, compute horizon-specific aligned predictions in one shot\n",
    "    for h in horizons:\n",
    "        # For each t, yhat_h[t] should be the model's prediction of x[t+h].\n",
    "        # A simple approximation from the above loop is to take x_gen shifted by -h.\n",
    "        yhat_h = torch.zeros_like(x)\n",
    "        yhat_h[:, :-h] = x_gen[:, h:]\n",
    "        yhat_h[:, -h:] = x_gen[:, -1:].expand(-1, h)  # dummy fill (unused in loss)\n",
    "        outs[h] = yhat_h\n",
    "    return outs\n",
    "\n",
    "def loss_step(model, batch_x, dt, horizons=(1,5,20), refresh_every=1,\n",
    "              lambda_tv=1e-3, lambda_l1=1e-4):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      - 1-step MSE (teacher-forced, direct forward)\n",
    "      - multi-horizon rollout MSE with refresh-every-k hybrid\n",
    "      - TV and L1 regularization on kernels\n",
    "    \"\"\"\n",
    "    y1, k, c, _ = model(batch_x, dt, return_kernel=True)   # teacher-forced pass\n",
    "    # 1-step alignment: y1[:, :-1] predicts x[:, 1:]\n",
    "    mse_1 = F.mse_loss(y1[:, :-1], batch_x[:, 1:])\n",
    "\n",
    "    # rollout losses\n",
    "    outs = rollout_multi_horizon(model, batch_x, dt, horizons, refresh_every=refresh_every)\n",
    "    mse_roll = 0.0\n",
    "    for h in horizons:\n",
    "        # only compare where target exists\n",
    "        mse_roll = mse_roll + F.mse_loss(outs[h][:, :-h], batch_x[:, h:])\n",
    "    mse_roll = mse_roll / len(horizons)\n",
    "\n",
    "    # regularization\n",
    "    reg = lambda_tv * total_variation_time(k) + lambda_l1 * l1_energy(k)\n",
    "    return mse_1, mse_roll, reg, {\"k\": k.detach()}\n",
    "\n",
    "# ----------------------------\n",
    "# Data prep: toy Lorenz x(t)\n",
    "# ----------------------------\n",
    "def lorenz(T_steps=30000, dt=0.005, sigma=10.0, rho=28.0, beta=8/3, x0=(1.0,1.0,1.0)):\n",
    "    x, y, z = x0\n",
    "    xs, ys, zs = [], [], []\n",
    "    for _ in range(T_steps):\n",
    "        def f(x,y,z):\n",
    "            dx = sigma*(y-x)\n",
    "            dy = x*(rho - z) - y\n",
    "            dz = x*y - beta*z\n",
    "            return dx, dy, dz\n",
    "        k1 = f(x,y,z)\n",
    "        k2 = f(x + 0.5*dt*k1[0], y + 0.5*dt*k1[1], z + 0.5*dt*k1[2])\n",
    "        k3 = f(x + 0.5*dt*k2[0], y + 0.5*dt*k2[1], z + 0.5*dt*k2[2])\n",
    "        k4 = f(x + dt*k3[0], y + dt*k3[1], z + dt*k3[2])\n",
    "        x += (dt/6.0)*(k1[0] + 2*k2[0] + 2*k3[0] + k4[0])\n",
    "        y += (dt/6.0)*(k1[1] + 2*k2[1] + 2*k3[1] + k4[1])\n",
    "        z += (dt/6.0)*(k1[2] + 2*k2[2] + 2*k3[2] + k4[2])\n",
    "        xs.append(x); ys.append(y); zs.append(z)\n",
    "    return np.array(xs), np.array(ys), np.array(zs)\n",
    "\n",
    "# ----------------------------\n",
    "# Minimal training loop (demo)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Generate toy data (replace with your real series) ---\n",
    "    dt0 = 0.005\n",
    "    xs, ys, zs = lorenz(T_steps=20000, dt=dt0)\n",
    "    x_np = xs.astype(np.float32)\n",
    "    # standardize (optional but helpful)\n",
    "    x_np = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "\n",
    "    # Train / test split\n",
    "    T = len(x_np)\n",
    "    split = int(0.8 * T)\n",
    "    x_train = to_tensor(x_np[:split][None, :], device)   # [B=1, T_tr]\n",
    "    x_test  = to_tensor(x_np[split:][None, :], device)   # [B=1, T_te]\n",
    "\n",
    "    # --- Build model ---\n",
    "    # Choose τ window in *seconds* (e.g., up to 0.5 s; tune to your data)\n",
    "    tau_min, tau_max = 0.0, 0.5\n",
    "    model = TVAROperator(L=128, tau_min=tau_min, tau_max=tau_max, n_modes=16, hidden=64).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-6)\n",
    "\n",
    "    # --- Train ---\n",
    "    horizons = (1, 5, 20)\n",
    "    refresh_every = 5   # hybrid rollout during training (matches your pipeline idea)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        mse1, mser, reg, extras = loss_step(model, x_train, dt0, horizons=horizons,\n",
    "                                            refresh_every=refresh_every,\n",
    "                                            lambda_tv=1e-3, lambda_l1=1e-4)\n",
    "        loss = mse1 + mser + reg\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"[{epoch+1:03d}] 1-step: {mse1.item():.4e}  roll:{mser.item():.4e}  reg:{reg.item():.4e}\")\n",
    "\n",
    "    # --- Evaluate on test at original dt ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Teacher-forced 1-step MSE\n",
    "        yhat_test = model(x_test, dt0)        # [1,Tte]\n",
    "        mse1_te = F.mse_loss(yhat_test[:, :-1], x_test[:, 1:]).item()\n",
    "\n",
    "        # Hybrid rollout with your knob (refresh-every-k)\n",
    "        k_refresh = 20\n",
    "        outs = rollout_multi_horizon(model, x_test, dt0, horizons=(1,5,20,50), refresh_every=k_refresh)\n",
    "        mse_roll_20 = F.mse_loss(outs[20][:, :-20], x_test[:, 20:]).item()\n",
    "        print(f\"Test 1-step MSE: {mse1_te:.4e}   20-step (refresh={k_refresh}) MSE: {mse_roll_20:.4e}\")\n",
    "\n",
    "    # --- (Key demo) Generalize to a different sampling rate ---\n",
    "    # Downsample by 2 -> dt' = 2*dt0 ; the *same model* is used, only dt changes.\n",
    "    with torch.no_grad():\n",
    "        x_coarse = x_np[::2]\n",
    "        x_coarse = to_tensor(((x_coarse - x_coarse.mean()) / (x_coarse.std() + 1e-8))[None, :], device)\n",
    "        dt1 = dt0 * 2.0\n",
    "        yhat_coarse = model(x_coarse, dt1)  # uses same τ-grid in seconds, resampled via interpolation\n",
    "        mse1_coarse = F.mse_loss(yhat_coarse[:, :-1], x_coarse[:, 1:]).item()\n",
    "        print(f\"Generalization to dt'={dt1:.4f}: 1-step MSE={mse1_coarse:.4e}\")\n",
    "\n",
    "    # --- Inspect learned kernel on test (optional) ---\n",
    "    with torch.no_grad():\n",
    "        _, k_test, _, _ = model(x_test, dt0, return_kernel=True)  # [1,Tte,L]\n",
    "        k_mean = k_test.mean(dim=1).squeeze(0).cpu().numpy()      # [L]\n",
    "        print(\"Kernel summary over τ (mean over time): mean=\", k_mean.mean(), \" std=\", k_mean.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367ea3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data prep: toy Lorenz x(t)\n",
    "# ----------------------------\n",
    "def lorenz(T_steps=30000, dt=0.005, sigma=10.0, rho=28.0, beta=8/3, x0=(1.0,1.0,1.0)):\n",
    "    x, y, z = x0\n",
    "    xs, ys, zs = [], [], []\n",
    "    for _ in range(T_steps):\n",
    "        def f(x,y,z):\n",
    "            dx = sigma*(y-x)\n",
    "            dy = x*(rho - z) - y\n",
    "            dz = x*y - beta*z\n",
    "            return dx, dy, dz\n",
    "        k1 = f(x,y,z)\n",
    "        k2 = f(x + 0.5*dt*k1[0], y + 0.5*dt*k1[1], z + 0.5*dt*k1[2])\n",
    "        k3 = f(x + 0.5*dt*k2[0], y + 0.5*dt*k2[1], z + 0.5*dt*k2[2])\n",
    "        k4 = f(x + dt*k3[0], y + dt*k3[1], z + dt*k3[2])\n",
    "        x += (dt/6.0)*(k1[0] + 2*k2[0] + 2*k3[0] + k4[0])\n",
    "        y += (dt/6.0)*(k1[1] + 2*k2[1] + 2*k3[1] + k4[1])\n",
    "        z += (dt/6.0)*(k1[2] + 2*k2[2] + 2*k3[2] + k4[2])\n",
    "        xs.append(x); ys.append(y); zs.append(z)\n",
    "    return np.array(xs), np.array(ys), np.array(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effc25fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 37\u001b[0m     mse1, mser, reg, extras \u001b[38;5;241m=\u001b[39m \u001b[43mloss_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlambda_tv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse1 \u001b[38;5;241m+\u001b[39m mser \u001b[38;5;241m+\u001b[39m reg\n\u001b[1;32m     41\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 283\u001b[0m, in \u001b[0;36mloss_step\u001b[0;34m(model, batch_x, dt, horizons, refresh_every, lambda_tv, lambda_l1)\u001b[0m\n\u001b[1;32m    280\u001b[0m mse_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y1[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], batch_x[:, \u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# rollout losses\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_multi_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m mse_roll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m horizons:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# only compare where target exists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 251\u001b[0m, in \u001b[0;36mrollout_multi_horizon\u001b[0;34m(model, x, dt, horizons, refresh_every)\u001b[0m\n\u001b[1;32m    246\u001b[0m     x_gen[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# predict next step using the operator at the *current* dt\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need a window covering (at least) the τ_max back in time; the model samples internally\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# For efficiency, we call model once per block; here we do a tiny call that returns the immediate next step.\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m yhat_step \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gen\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, t+1]\u001b[39;00m\n\u001b[1;32m    252\u001b[0m next_pred \u001b[38;5;241m=\u001b[39m yhat_step[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]                       \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# write predicted (open-loop) step into x_gen\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 217\u001b[0m, in \u001b[0;36mTVAROperator.forward\u001b[0;34m(self, x, dt, t_offset, return_kernel)\u001b[0m\n\u001b[1;32m    213\u001b[0m delta_tau \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_min) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    215\u001b[0m yhat \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m Xlags)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m delta_tau \u001b[38;5;241m+\u001b[39m c        \u001b[38;5;66;03m# [B,T]\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_kernel:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m yhat, k, c, Xlags\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m yhat\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Minimal training loop (demo)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # if MPS is flaky, force CPU once to confirm it’s your code, not the backend:\n",
    "    # device = \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "\n",
    "    # --- Generate toy data (replace with your real series) ---\n",
    "    dt0 = 0.005\n",
    "    xs, ys, zs = lorenz(T_steps=20000, dt=dt0)\n",
    "    x_np = xs.astype(np.float32)\n",
    "    # standardize (optional but helpful)\n",
    "    x_np = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "\n",
    "    # Train / test split\n",
    "    T = len(x_np)\n",
    "    split = int(0.8 * T)\n",
    "    x_train = to_tensor(x_np[:split][None, :], device)   # [B=1, T_tr]\n",
    "    x_test  = to_tensor(x_np[split:][None, :], device)   # [B=1, T_te]\n",
    "\n",
    "    # --- Build model ---\n",
    "    # Choose τ window in *seconds* (e.g., up to 0.5 s; tune to your data)\n",
    "    tau_min, tau_max = dt0, 0.5\n",
    "    model = TVAROperator(L=128, tau_min=tau_min, tau_max=tau_max, n_modes=16, hidden=64).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-6)\n",
    "\n",
    "    # --- Train ---\n",
    "    horizons = (1, 5, 20)\n",
    "    refresh_every = 5   # hybrid rollout during training (matches your pipeline idea)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        mse1, mser, reg, extras = loss_step(model, x_train, dt0, horizons=horizons,\n",
    "                                            refresh_every=refresh_every,\n",
    "                                            lambda_tv=1e-3, lambda_l1=1e-4)\n",
    "        loss = mse1 + mser + reg\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"[{epoch+1:03d}] 1-step: {mse1.item():.4e}  roll:{mser.item():.4e}  reg:{reg.item():.4e}\")\n",
    "\n",
    "    # --- Evaluate on test at original dt ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Teacher-forced 1-step MSE\n",
    "        yhat_test = model(x_test, dt0)        # [1,Tte]\n",
    "        mse1_te = F.mse_loss(yhat_test[:, :-1], x_test[:, 1:]).item()\n",
    "\n",
    "        # Hybrid rollout with your knob (refresh-every-k)\n",
    "        k_refresh = 20\n",
    "        outs = rollout_multi_horizon(model, x_test, dt0, horizons=(1,5,20,50), refresh_every=k_refresh)\n",
    "        mse_roll_20 = F.mse_loss(outs[20][:, :-20], x_test[:, 20:]).item()\n",
    "        print(f\"Test 1-step MSE: {mse1_te:.4e}   20-step (refresh={k_refresh}) MSE: {mse_roll_20:.4e}\")\n",
    "\n",
    "    # --- (Key demo) Generalize to a different sampling rate ---\n",
    "    # Downsample by 2 -> dt' = 2*dt0 ; the *same model* is used, only dt changes.\n",
    "    with torch.no_grad():\n",
    "        x_coarse = x_np[::2]\n",
    "        x_coarse = to_tensor(((x_coarse - x_coarse.mean()) / (x_coarse.std() + 1e-8))[None, :], device)\n",
    "        dt1 = dt0 * 2.0\n",
    "        yhat_coarse = model(x_coarse, dt1)  # uses same τ-grid in seconds, resampled via interpolation\n",
    "        mse1_coarse = F.mse_loss(yhat_coarse[:, :-1], x_coarse[:, 1:]).item()\n",
    "        print(f\"Generalization to dt'={dt1:.4f}: 1-step MSE={mse1_coarse:.4e}\")\n",
    "\n",
    "    # --- Inspect learned kernel on test (optional) ---\n",
    "    with torch.no_grad():\n",
    "        _, k_test, _, _ = model(x_test, dt0, return_kernel=True)  # [1,Tte,L]\n",
    "        k_mean = k_test.mean(dim=1).squeeze(0).cpu().numpy()      # [L]\n",
    "        print(\"Kernel summary over τ (mean over time): mean=\", k_mean.mean(), \" std=\", k_mean.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0928dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] 1-step 1.0390e+00 | roll 1.1882e-02 | reg 1.5435e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [04:57<19:49, 297.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] 1-step 9.7421e-01 | roll 1.1617e-02 | reg 1.6640e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [06:04<08:05, 161.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] 1-step 9.1016e-01 | roll 1.1343e-02 | reg 1.7920e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3/5 [07:11<03:56, 118.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] 1-step 8.4813e-01 | roll 1.1066e-02 | reg 1.9290e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4/5 [08:15<01:37, 97.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] 1-step 7.8913e-01 | roll 1.0786e-02 | reg 2.0719e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [09:19<00:00, 111.88s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 59\u001b[0m     yhat_te \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     mse1_te \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(yhat_te[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x_test[:, \u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m     outs \u001b[38;5;241m=\u001b[39m rollout_multi_horizon(model, x_test, dt0, horizons\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m), refresh_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 244\u001b[0m, in \u001b[0;36mTVAROperator.forward\u001b[0;34m(self, x, dt, t_offset, return_kernel)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Riemann sum over τ (Δτ constant)  <-- define BEFORE yhat\u001b[39;00m\n\u001b[1;32m    242\u001b[0m delta_tau \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_min) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 244\u001b[0m yhat \u001b[38;5;241m=\u001b[39m (\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXlags\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m delta_tau \u001b[38;5;241m+\u001b[39m c        \u001b[38;5;66;03m# [B,T]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_kernel:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m yhat, k, c, Xlags\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Tiny debug config ---\n",
    "dt0 = 0.05\n",
    "# use only a small slice initially\n",
    "x_np_small = x_np[:400].astype(np.float32)\n",
    "x_train = to_tensor(x_np_small[:3200][None, :], device)\n",
    "x_test  = to_tensor(x_np_small[3200:][None, :], device)\n",
    "\n",
    "model = TVAROperator(\n",
    "    L=32,\n",
    "    tau_min=dt0,      # avoid τ=0 for forecasting\n",
    "    tau_max=0.25,\n",
    "    n_modes=8,\n",
    "    hidden=16\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "horizons = (1, 5, 10)\n",
    "refresh_every = 5\n",
    "\n",
    "# Optional: keep kernels small & stable\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Forward smoke test\n",
    "with torch.no_grad():\n",
    "    yhat, k, c, Xlags = model(x_train, dt0, return_kernel=True)\n",
    "    assert yhat.shape == x_train.shape\n",
    "    assert k.shape == Xlags.shape == (x_train.size(0), x_train.size(1), model.L)\n",
    "    assert c.shape == (x_train.size(0), x_train.size(1))\n",
    "    for name, t in [(\"yhat\", yhat), (\"k\", k), (\"c\", c), (\"Xlags\", Xlags)]:\n",
    "        assert torch.isfinite(t).all(), f\"non-finite in {name}\"\n",
    "\n",
    "# Tiny training loop\n",
    "LOG_EVERY = 1\n",
    "for epoch in tqdm(range(1, 6), desc=\"Training\"):\n",
    "# for epoch in range(5):\n",
    "    model.train()\n",
    "    mse1, mser, reg, _ = loss_step(\n",
    "        model, x_train, dt0,\n",
    "        horizons=horizons,\n",
    "        refresh_every=refresh_every,\n",
    "        lambda_tv=1e-3, lambda_l1=1e-4\n",
    "    )\n",
    "    # guard against NaNs\n",
    "    for name, t in [(\"mse1\", mse1), (\"mser\", mser), (\"reg\", reg)]:\n",
    "        if not torch.isfinite(t):\n",
    "            raise RuntimeError(f\"non-finite {name}: {t.item()}\")\n",
    "    loss = mse1 + mser + reg\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    if (epoch + 1) % LOG_EVERY == 0 or epoch == 0:\n",
    "        print(f\"[{epoch+1}] 1-step {mse1.item():.4e} | roll {mser.item():.4e} | reg {reg.item():.4e}\", flush=True)\n",
    "# Quick eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_te = model(x_test, dt0)\n",
    "    mse1_te = F.mse_loss(yhat_te[:, :-1], x_test[:, 1:]).item()\n",
    "    outs = rollout_multi_horizon(model, x_test, dt0, horizons=(1,5,10), refresh_every=10)\n",
    "    mse10 = F.mse_loss(outs[10][:, :-10], x_test[:, 10:]).item()\n",
    "    print(f\"Test 1-step: {mse1_te:.4e} | 10-step hybrid: {mse10:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48938c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 27\u001b[0m     mse1, mser, reg, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlambda_tv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_ROLLOUT:\n\u001b[1;32m     31\u001b[0m         mser \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m*\u001b[39m mse1\n",
      "Cell \u001b[0;32mIn[3], line 283\u001b[0m, in \u001b[0;36mloss_step\u001b[0;34m(model, batch_x, dt, horizons, refresh_every, lambda_tv, lambda_l1)\u001b[0m\n\u001b[1;32m    280\u001b[0m mse_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y1[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], batch_x[:, \u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# rollout losses\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_multi_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m mse_roll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m horizons:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# only compare where target exists\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 251\u001b[0m, in \u001b[0;36mrollout_multi_horizon\u001b[0;34m(model, x, dt, horizons, refresh_every)\u001b[0m\n\u001b[1;32m    246\u001b[0m     x_gen[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x[:, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# predict next step using the operator at the *current* dt\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need a window covering (at least) the τ_max back in time; the model samples internally\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# For efficiency, we call model once per block; here we do a tiny call that returns the immediate next step.\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m yhat_step \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gen\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, t+1]\u001b[39;00m\n\u001b[1;32m    252\u001b[0m next_pred \u001b[38;5;241m=\u001b[39m yhat_step[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]                       \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# write predicted (open-loop) step into x_gen\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 208\u001b[0m, in \u001b[0;36mTVAROperator.forward\u001b[0;34m(self, x, dt, t_offset, return_kernel)\u001b[0m\n\u001b[1;32m    205\u001b[0m c  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias(hT)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                    \u001b[38;5;66;03m# [B,T]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# sample lagged signal at continuous τ-grid\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m Xlags \u001b[38;5;241m=\u001b[39m \u001b[43mfractional_delay_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_offset\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m                                                 \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Riemann sum over τ (Δτ constant)  <-- define BEFORE yhat\u001b[39;00m\n\u001b[1;32m    213\u001b[0m delta_tau \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_min) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mfractional_delay_samples\u001b[0;34m(x, tau_grid, dt, t_offset)\u001b[0m\n\u001b[1;32m     81\u001b[0m src0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(torch\u001b[38;5;241m.\u001b[39mfloor(src), \u001b[38;5;241m0\u001b[39m, T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)          \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[1;32m     82\u001b[0m src1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(src0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,          \u001b[38;5;241m0\u001b[39m, T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)                        \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m w    \u001b[38;5;241m=\u001b[39m (src \u001b[38;5;241m-\u001b[39m \u001b[43msrc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(dtype)                                 \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# gather values\u001b[39;00m\n\u001b[1;32m     86\u001b[0m idx0 \u001b[38;5;241m=\u001b[39m src0\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                                          \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data\n",
    "dt0 = 0.005\n",
    "xs, ys, zs = lorenz(T_steps=8000, dt=dt0)  # ↓\n",
    "x_np = ((xs - xs.mean()) / (xs.std() + 1e-8)).astype(np.float32)\n",
    "split = int(0.8 * len(x_np))\n",
    "x_train = to_tensor(x_np[:split][None, :], device)\n",
    "x_test  = to_tensor(x_np[split:][None, :], device)\n",
    "\n",
    "# Model\n",
    "model = TVAROperator(\n",
    "    L=32,            # ↓ 16 if you need more speed\n",
    "    tau_min=dt0,\n",
    "    tau_max=0.25,    # ↓ 0.15 for more speed\n",
    "    n_modes=8,       # ↓ 4\n",
    "    hidden=16        # ↓ 8\n",
    ").to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "# Training\n",
    "epochs = 5\n",
    "horizons = (1,)     # training only; see USE_ROLLOUT below\n",
    "refresh_every = 10\n",
    "USE_ROLLOUT = False # ← main speed win\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    mse1, mser, reg, _ = loss_step(model, x_train, dt0,\n",
    "                                   horizons=horizons, refresh_every=refresh_every,\n",
    "                                   lambda_tv=1e-3, lambda_l1=1e-4)\n",
    "    if not USE_ROLLOUT:\n",
    "        mser = 0.0 * mse1\n",
    "    loss = mse1 + mser + reg\n",
    "    opt.zero_grad(set_to_none=True); loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
    "    print(f\"[{epoch}] 1step={mse1.item():.3e} reg={reg.item():.3e}\")\n",
    "\n",
    "# Eval (run rollout here if you want)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_te = model(x_test, dt0)\n",
    "    mse1_te = F.mse_loss(yhat_te[:, :-1], x_test[:, 1:]).item()\n",
    "    outs = rollout_multi_horizon(model, x_test, dt0,\n",
    "                                 horizons=(1,5,10), refresh_every=20)  # bigger refresh\n",
    "    mse10 = F.mse_loss(outs[10][:, :-10], x_test[:, 10:]).item()\n",
    "    print(f\"Test 1-step={mse1_te:.3e} | 10-step hybrid={mse10:.3e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
