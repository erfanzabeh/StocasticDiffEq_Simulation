{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4aee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b570d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows_from_dataset(ds, p_max=20):\n",
    "    \"\"\"\n",
    "    ds: dict with keys X [N,T], A [N,T,p_max_data], p_true [N]\n",
    "    Returns windowed arrays:\n",
    "      Z: [M, p_max]\n",
    "      y: [M]\n",
    "      a_true: [M, p_max]     (coeff targets aligned to y times)\n",
    "      p_true_win: [M]        (order label per window)\n",
    "      sample_id: [M]         (which trajectory each window came from)\n",
    "      time_idx: [M]          (absolute t index in the original series)\n",
    "    \"\"\"\n",
    "    X = ds[\"X\"]\n",
    "    A = ds[\"A\"]\n",
    "    p_true = ds[\"p_true\"]\n",
    "\n",
    "    N, T = X.shape\n",
    "    p_max_data = A.shape[2]  # infer from data\n",
    "    assert A.shape == (N, T, p_max_data)\n",
    "\n",
    "    M_per = T - p_max\n",
    "    M = N * M_per\n",
    "\n",
    "    Z = np.zeros((M, p_max), dtype=np.float32)\n",
    "    y = np.zeros((M,), dtype=np.float32)\n",
    "    a_true = np.zeros((M, p_max), dtype=np.float32)\n",
    "    p_true_win = np.zeros((M,), dtype=np.int64)\n",
    "    sample_id = np.zeros((M,), dtype=np.int64)\n",
    "    time_idx = np.zeros((M,), dtype=np.int64)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(N):\n",
    "        x = X[i]\n",
    "        # windows: t runs p_max..T-1\n",
    "        for t in range(p_max, T):\n",
    "            Z[idx] = x[t-p_max:t][::-1]      # [x_{t-1},...,x_{t-p_max}]\n",
    "            y[idx] = x[t]\n",
    "            # take only the first p_max coefficients (or pad if p_max > p_max_data)\n",
    "            a_true[idx, :min(p_max, p_max_data)] = A[i, t, :min(p_max, p_max_data)]\n",
    "            p_true_win[idx] = int(p_true[i]) # same label for all windows in traj\n",
    "            sample_id[idx] = i\n",
    "            time_idx[idx] = t\n",
    "            idx += 1\n",
    "\n",
    "    return Z, y, a_true, p_true_win, sample_id, time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd8f983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trajectories(pilot, train_frac=0.6, val_frac=0.2, seed=0, method=\"global\"):\n",
    "    \"\"\"\n",
    "    Returns trajectory indices: tr_ids, va_ids, te_ids\n",
    "\n",
    "    method:\n",
    "      - \"global\": random split over all trajectories (Option 2)\n",
    "      - \"by_family\": split within each family to preserve proportions (Option 1)\n",
    "    \"\"\"\n",
    "    X = pilot[\"X\"]\n",
    "    N = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    assert 0 < train_frac < 1\n",
    "    assert 0 <= val_frac < 1\n",
    "    assert train_frac + val_frac < 1\n",
    "\n",
    "    if method == \"global\":\n",
    "        perm = rng.permutation(N)\n",
    "        n_tr = int(round(train_frac * N))\n",
    "        n_va = int(round(val_frac * N))\n",
    "        tr_ids = perm[:n_tr]\n",
    "        va_ids = perm[n_tr:n_tr + n_va]\n",
    "        te_ids = perm[n_tr + n_va:]\n",
    "        return tr_ids, va_ids, te_ids\n",
    "\n",
    "    if method == \"by_family\":\n",
    "        if \"class_id\" not in pilot or pilot[\"class_id\"] is None:\n",
    "            raise ValueError(\"pilot must contain 'class_id' for method='by_family'\")\n",
    "\n",
    "        class_id = pilot[\"class_id\"]\n",
    "        tr_ids, va_ids, te_ids = [], [], []\n",
    "\n",
    "        for cid in np.unique(class_id):\n",
    "            ids = np.where(class_id == cid)[0]\n",
    "            perm = rng.permutation(ids)\n",
    "\n",
    "            n = len(ids)\n",
    "            n_tr = int(round(train_frac * n))\n",
    "            n_va = int(round(val_frac * n))\n",
    "\n",
    "            tr_ids.extend(perm[:n_tr].tolist())\n",
    "            va_ids.extend(perm[n_tr:n_tr + n_va].tolist())\n",
    "            te_ids.extend(perm[n_tr + n_va:].tolist())\n",
    "\n",
    "        # shuffle within splits so batches mix families\n",
    "        tr_ids = np.array(tr_ids, dtype=int)\n",
    "        va_ids = np.array(va_ids, dtype=int)\n",
    "        te_ids = np.array(te_ids, dtype=int)\n",
    "\n",
    "        tr_ids = rng.permutation(tr_ids)\n",
    "        va_ids = rng.permutation(va_ids)\n",
    "        te_ids = rng.permutation(te_ids)\n",
    "        return tr_ids, va_ids, te_ids\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30f019fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_pilot(pilot, ids):\n",
    "    out = {\n",
    "        \"X\": pilot[\"X\"][ids],\n",
    "        \"A\": pilot[\"A\"][ids],\n",
    "        \"p_true\": pilot[\"p_true\"][ids],\n",
    "    }\n",
    "    # optional fields if present\n",
    "    if \"class_id\" in pilot and pilot[\"class_id\"] is not None:\n",
    "        out[\"class_id\"] = pilot[\"class_id\"][ids]\n",
    "    if \"noise_std\" in pilot and pilot[\"noise_std\"] is not None:\n",
    "        out[\"noise_std\"] = pilot[\"noise_std\"][ids]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c1d8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTVAR_MultiHead(nn.Module):\n",
    "    def __init__(self, p_max, hidden=128, depth=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = p_max\n",
    "        for _ in range(depth):\n",
    "            layers += [\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(hidden),\n",
    "                nn.Dropout(dropout),\n",
    "            ]\n",
    "            in_dim = hidden\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        self.coeff_head = nn.Linear(hidden, p_max)   # coeffs in standardized space\n",
    "        self.bias_head  = nn.Linear(hidden, 1)       # bias in standardized space\n",
    "        self.p_head     = nn.Linear(hidden, p_max)   # logits for p in {1..p_max}\n",
    "\n",
    "        self.p_max = p_max\n",
    "\n",
    "    def forward(self, Zs):\n",
    "        h = self.backbone(Zs)\n",
    "        coeffs_s = self.coeff_head(h)                # [B, p_max]\n",
    "        bias_s = self.bias_head(h).squeeze(-1)       # [B]\n",
    "        p_logits = self.p_head(h)                    # [B, p_max]\n",
    "\n",
    "        y_pred_s = (coeffs_s * Zs).sum(dim=1) + bias_s\n",
    "        return y_pred_s, coeffs_s, bias_s, p_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67eba9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_tvar(\n",
    "    train_ds, val_ds,\n",
    "    p_max=20,\n",
    "    hidden=128, depth=3, dropout=0.05,\n",
    "    batch_size=512,\n",
    "    epochs=30,\n",
    "    lr=2e-3,\n",
    "    weight_decay=1e-4,\n",
    "    coeff_loss_w=0.2,     # how hard to match true coefficients\n",
    "    order_loss_w=0.2,     # how hard to predict p_true\n",
    "    l1_out_w=1e-5,        # L1 on output coeffs (encourages sparsity)\n",
    "    eval_every=1,\n",
    "    patience=5,\n",
    "    seed=0,\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Windowize\n",
    "    Z_tr, y_tr, a_tr, p_tr, sid_tr, t_tr = make_windows_from_dataset(train_ds, p_max=p_max)\n",
    "    Z_va, y_va, a_va, p_va, sid_va, t_va = make_windows_from_dataset(val_ds,   p_max=p_max)\n",
    "\n",
    "    # Standardize using TRAIN windows only\n",
    "    Z_mean = Z_tr.mean(axis=0, keepdims=True)\n",
    "    Z_std  = Z_tr.std(axis=0, keepdims=True) + 1e-8\n",
    "    y_mean = y_tr.mean()\n",
    "    y_std  = y_tr.std() + 1e-8\n",
    "\n",
    "    Z_tr_s = (Z_tr - Z_mean) / Z_std\n",
    "    Z_va_s = (Z_va - Z_mean) / Z_std\n",
    "    y_tr_s = (y_tr - y_mean) / y_std\n",
    "    y_va_s = (y_va - y_mean) / y_std\n",
    "\n",
    "    # Coeff targets in standardized space:\n",
    "    # coeffs_s_k = a_k * Z_std_k / y_std\n",
    "    a_tr_s = a_tr * (Z_std.reshape(-1) / y_std)\n",
    "    a_va_s = a_va * (Z_std.reshape(-1) / y_std)\n",
    "\n",
    "    # p labels: map 1..p_max -> 0..p_max-1\n",
    "    p_tr_cls = (p_tr - 1).astype(np.int64)\n",
    "    p_va_cls = (p_va - 1).astype(np.int64)\n",
    "\n",
    "    # Torch datasets/loaders\n",
    "    tr_loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(Z_tr_s, dtype=torch.float32),\n",
    "            torch.tensor(y_tr_s, dtype=torch.float32),\n",
    "            torch.tensor(a_tr_s, dtype=torch.float32),\n",
    "            torch.tensor(p_tr_cls, dtype=torch.long),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    va_loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(Z_va_s, dtype=torch.float32),\n",
    "            torch.tensor(y_va_s, dtype=torch.float32),\n",
    "            torch.tensor(a_va_s, dtype=torch.float32),\n",
    "            torch.tensor(p_va_cls, dtype=torch.long),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    model = MLPTVAR_MultiHead(p_max=p_max, hidden=hidden, depth=depth, dropout=dropout).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    ce  = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for Zb, yb, ab, pb in tr_loader:\n",
    "            Zb = Zb.to(device); yb = yb.to(device); ab = ab.to(device); pb = pb.to(device)\n",
    "\n",
    "            yhat_s, coeffs_s, bias_s, p_logits = model(Zb)\n",
    "\n",
    "            loss_y = mse(yhat_s, yb)\n",
    "            loss_a = mse(coeffs_s, ab)\n",
    "            loss_p = ce(p_logits, pb)\n",
    "            loss_l1 = coeffs_s.abs().mean()\n",
    "\n",
    "            loss = loss_y + coeff_loss_w * loss_a + order_loss_w * loss_p + l1_out_w * loss_l1\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss += float(loss.item()) * Zb.size(0)\n",
    "\n",
    "        tr_loss /= len(tr_loader.dataset)\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            model.eval()\n",
    "            va_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Zb, yb, ab, pb in va_loader:\n",
    "                    Zb = Zb.to(device); yb = yb.to(device); ab = ab.to(device); pb = pb.to(device)\n",
    "                    yhat_s, coeffs_s, bias_s, p_logits = model(Zb)\n",
    "\n",
    "                    loss_y = mse(yhat_s, yb)\n",
    "                    loss_a = mse(coeffs_s, ab)\n",
    "                    loss_p = ce(p_logits, pb)\n",
    "                    loss_l1 = coeffs_s.abs().mean()\n",
    "\n",
    "                    loss = loss_y + coeff_loss_w * loss_a + order_loss_w * loss_p + l1_out_w * loss_l1\n",
    "                    va_loss += float(loss.item()) * Zb.size(0)\n",
    "\n",
    "            va_loss /= len(va_loader.dataset)\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | train={tr_loss:.6f} | val={va_loss:.6f}\")\n",
    "\n",
    "            if va_loss < best_val - 1e-6:\n",
    "                best_val = va_loss\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best val={best_val:.6f}\")\n",
    "                    break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    pack = {\n",
    "        \"model\": model,\n",
    "        \"device\": device,\n",
    "        \"Z_mean\": Z_mean.astype(np.float32),\n",
    "        \"Z_std\":  Z_std.astype(np.float32),\n",
    "        \"y_mean\": float(y_mean),\n",
    "        \"y_std\":  float(y_std),\n",
    "        \"p_max\":  int(p_max),\n",
    "    }\n",
    "    return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6353992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(pack, test_ds):\n",
    "    model = pack[\"model\"]\n",
    "    device = pack[\"device\"]\n",
    "    Z_mean = pack[\"Z_mean\"]\n",
    "    Z_std  = pack[\"Z_std\"]\n",
    "    y_mean = pack[\"y_mean\"]\n",
    "    y_std  = pack[\"y_std\"]\n",
    "    p_max  = pack[\"p_max\"]\n",
    "\n",
    "    Z_te, y_te, a_te, p_te, sid_te, t_te = make_windows_from_dataset(test_ds, p_max=p_max)\n",
    "\n",
    "    # standardize features\n",
    "    Z_te_s = (Z_te - Z_mean) / Z_std\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Zt = torch.tensor(Z_te_s, dtype=torch.float32).to(device)\n",
    "        yhat_s, coeffs_s, bias_s, p_logits = model(Zt)\n",
    "\n",
    "        yhat_s = yhat_s.cpu().numpy()\n",
    "        coeffs_s = coeffs_s.cpu().numpy()\n",
    "        p_logits = p_logits.cpu().numpy()\n",
    "\n",
    "    # unstandardize predictions\n",
    "    yhat = yhat_s * y_std + y_mean\n",
    "\n",
    "    # unstandardize coefficients to original AR coefficients:\n",
    "    # a_hat = coeffs_s * y_std / Z_std\n",
    "    a_hat = coeffs_s * (y_std / Z_std.reshape(-1))\n",
    "\n",
    "    # window-level prediction mse\n",
    "    pred_mse = float(np.mean((yhat - y_te) ** 2))\n",
    "\n",
    "    # trajectory-level order prediction: aggregate logits per trajectory\n",
    "    N = test_ds[\"X\"].shape[0]\n",
    "    logits_sum = np.zeros((N, p_max), dtype=np.float64)\n",
    "    counts = np.zeros((N,), dtype=np.int64)\n",
    "\n",
    "    for i in range(len(sid_te)):\n",
    "        sid = sid_te[i]\n",
    "        logits_sum[sid] += p_logits[i]\n",
    "        counts[sid] += 1\n",
    "\n",
    "    logits_mean = logits_sum / np.maximum(counts[:, None], 1)\n",
    "    p_hat = np.argmax(logits_mean, axis=1) + 1  # back to 1..p_max\n",
    "\n",
    "    p_true_traj = test_ds[\"p_true\"].astype(int)\n",
    "    order_acc = float(np.mean(p_hat == p_true_traj))\n",
    "\n",
    "    # coefficient MSE masked to active lags per trajectory\n",
    "    # We compute per-window mse with mask determined by that trajectory's p_true.\n",
    "    masks = np.zeros((N, p_max), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        masks[i, :p_true_traj[i]] = 1.0\n",
    "\n",
    "    # apply window-wise mask\n",
    "    mask_w = masks[sid_te]  # [M, p_max]\n",
    "    coeff_mse_masked = float(np.sum(((a_hat - a_te) ** 2) * mask_w) / (np.sum(mask_w) + 1e-8))\n",
    "\n",
    "    out = {\n",
    "        \"pred_mse\": pred_mse,\n",
    "        \"order_acc\": order_acc,\n",
    "        \"coeff_mse_masked\": coeff_mse_masked,\n",
    "        \"p_hat\": p_hat,\n",
    "        \"p_true\": p_true_traj,\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dcef3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot = np.load(\"tvar_pilot_T10000.npz\")\n",
    "\n",
    "# 1) Split by trajectory\n",
    "tr_ids, va_ids, te_ids = split_trajectories(pilot, method=\"global\", seed=0)\n",
    "\n",
    "train_ds = subset_pilot(pilot, tr_ids)\n",
    "val_ds   = subset_pilot(pilot, va_ids)\n",
    "test_ds  = subset_pilot(pilot, te_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd1a4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train=1.313946 | val=1.285309\n",
      "Epoch 002 | train=1.261396 | val=1.290521\n",
      "Epoch 003 | train=1.259574 | val=1.288601\n",
      "Epoch 004 | train=1.258712 | val=1.290433\n",
      "Epoch 005 | train=1.258545 | val=1.290193\n",
      "Epoch 006 | train=1.258485 | val=1.286990\n",
      "Early stopping at epoch 6. Best val=1.285309\n",
      "{'pred_mse': 0.11363424360752106, 'order_acc': 0.4375, 'coeff_mse_masked': 0.023323909327213517, 'p_hat': array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]), 'p_true': array([4, 4, 4, 2, 4, 6, 4, 4, 1, 4, 6, 2, 1, 1, 2, 2])}\n"
     ]
    }
   ],
   "source": [
    "# 2) Train the MLP baseline\n",
    "pack = train_mlp_tvar(\n",
    "    train_ds, val_ds,\n",
    "    p_max=6,\n",
    "    hidden=128, depth=3, dropout=0.05,\n",
    "    batch_size=512,\n",
    "    epochs=30,\n",
    "    lr=2e-3,\n",
    "    coeff_loss_w=0.2,\n",
    "    order_loss_w=0.2,\n",
    "    l1_out_w=1e-5,\n",
    "    patience=5,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# 3) Evaluate\n",
    "metrics = evaluate_on_test(pack, test_ds)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "armodeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
