{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ec9503",
   "metadata": {},
   "source": [
    "# TVAR PINN with PyTorch\n",
    "This notebook demonstrates how to use a modern automatic‑differentiation framework (PyTorch)\n",
    "to learn time‑varying autoregressive (TVAR) coefficients for a simulated signal with constant\n",
    "sliding‑window power.  The goal is to illustrate how automatic differentiation and an optimiser\n",
    "such as Adam can replace the manual gradient calculations used previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9028daa",
   "metadata": {},
   "source": [
    "## Background\n",
    "TVAR models describe signals whose autoregressive coefficients change over time.  Estimating these coefficients\n",
    "is challenging, particularly when only weak constraints such as constant sliding‑window power are available;\n",
    "multiple coefficient trajectories can reproduce the same signal【988150528581547†L918-L933】.  We use a physics‑informed neural network\n",
    "(PINN) to embed prior knowledge into the loss function: an autoregressive residual term, an energy penalty\n",
    "to enforce constant power, and a smoothness regulariser.  Unlike the earlier implementation, the neural network\n",
    "and its gradients are implemented using PyTorch, which handles automatic differentiation【672647501557241†L152-L160】.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7b7ad",
   "metadata": {},
   "source": [
    "## Signal Simulation (NumPy)\n",
    "First, we simulate a one‑dimensional time series governed by a TVAR(2) process.  The underlying AR coefficients\n",
    "vary slowly and produce low‑frequency oscillatory dynamics reminiscent of local field potentials (LFPs).\n",
    "The signal is scaled to maintain constant sliding‑window power.\n",
    "\n",
    "Run the following code cell to generate the simulated signal and display its sliding‑window power.  Adjust the\n",
    "`freq1`, `freq2`, `P0`, and `W` parameters to modify the underlying dynamics and the window size used for the power constraint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90234210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation parameters\n",
    "N = 600  # length of the time series\n",
    "p = 2    # autoregressive order\n",
    "W = 40   # window length for constant power\n",
    "P0 = 0.5 # target sliding‑window power\n",
    "sigma_noise = 0.1\n",
    "\n",
    "# Frequencies for slowly varying coefficients\n",
    "freq1 = 1/150\n",
    "freq2 = 1/200\n",
    "\n",
    "# Functions generating true AR coefficients\n",
    "a1_func = lambda t: 0.6 + 0.35 * np.sin(2 * np.pi * freq1 * t)\n",
    "a2_func = lambda t: -0.5 + 0.25 * np.cos(2 * np.pi * freq2 * t)\n",
    "\n",
    "# Generate coefficients over time\n",
    "coeffs_true = np.zeros((N, p))\n",
    "for t in range(N):\n",
    "    coeffs_true[t, 0] = a1_func(t)\n",
    "    coeffs_true[t, 1] = a2_func(t)\n",
    "\n",
    "# Simulate the TVAR signal and enforce constant window power\n",
    "x = np.zeros(N)\n",
    "np.random.seed(42)\n",
    "for i in range(N):\n",
    "    # base white noise\n",
    "    val = np.random.normal(scale=sigma_noise)\n",
    "    # autoregressive component\n",
    "    for k in range(1, p+1):\n",
    "        if i >= k:\n",
    "            val += coeffs_true[i, k-1] * x[i-k]\n",
    "    x[i] = val\n",
    "    # enforce power constraint after W samples\n",
    "    if i >= W-1:\n",
    "        window = x[i-W+1:i+1]\n",
    "        current_power = np.mean(window**2)\n",
    "        s = np.sqrt(P0 / current_power) if current_power > 0 else 1.0\n",
    "        # clip scaling factor to avoid extreme values\n",
    "        s = np.clip(s, 0.8, 1.2)\n",
    "        x[i] *= s\n",
    "\n",
    "# Compute sliding‑window power for plotting\n",
    "power = np.zeros(N)\n",
    "for i in range(N):\n",
    "    start = max(0, i-W+1)\n",
    "    window = x[start:i+1]\n",
    "    power[i] = np.mean(window**2)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "ax[0].plot(x)\n",
    "ax[0].set_ylabel('Amplitude')\n",
    "ax[0].set_title('Simulated TVAR Signal')\n",
    "ax[1].plot(power, color='tab:orange')\n",
    "ax[1].axhline(P0, color='k', linestyle='--')\n",
    "ax[1].set_xlabel('Sample index')\n",
    "ax[1].set_ylabel('Power')\n",
    "ax[1].set_title('Sliding‑Window Power')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6a473",
   "metadata": {},
   "source": [
    "## PINN Definition and Training with PyTorch\n",
    "We now define a simple feed‑forward neural network using PyTorch.  The network takes a normalised time coordinate\n",
    "`t_norm` as input and outputs two values corresponding to the AR coefficients at that time.  A scaling factor\n",
    "`scale_coeff` keeps the raw network outputs within a range that tends to yield stable AR processes.\n",
    "\n",
    "The loss function consists of three terms:\n",
    "1. **Autoregressive residual** between the observed signal and the one‑step prediction using the predicted coefficients.\n",
    "2. **Energy penalty** encouraging the sliding‑window power of the predicted signal to match the target `P0`.\n",
    "3. **Smoothness penalty** on changes in the predicted coefficients over time.\n",
    "\n",
    "We train the network using the Adam optimiser.  Note that PyTorch must be installed to run this cell; the code\n",
    "is provided here as a template for execution on systems with PyTorch available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b03f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyTorch implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure that a GPU is used if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "x_torch = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "coeffs_true_torch = torch.tensor(coeffs_true, dtype=torch.float32, device=device)\n",
    "\n",
    "# Create normalized time input\n",
    "t_norm = torch.linspace(0.0, 1.0, N, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "class TVARPINN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=40, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.tanh\n",
    "    def forward(self, t):\n",
    "        x = self.activation(self.fc1(t))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 40\n",
    "scale_coeff = 0.5\n",
    "lambda_energy = 0.1\n",
    "lambda_smooth = 0.01\n",
    "learning_rate = 0.01\n",
    "n_epochs = 300\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = TVARPINN(input_dim=1, hidden_dim=hidden_dim, output_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Predict coefficients for all time points, then select those for indices >= p\n",
    "    coeff_pred = model(t_norm) * scale_coeff\n",
    "    coeff_pred_use = coeff_pred[p:]\n",
    "    \n",
    "    # Compute one‑step predictions using the predicted coefficients\n",
    "    M = N - p\n",
    "    x_hat = []\n",
    "    for i in range(p, N):\n",
    "        past_vals = torch.tensor([x_torch[i-1], x_torch[i-2]], dtype=torch.float32, device=device)\n",
    "        coeffs = coeff_pred_use[i - p]\n",
    "        x_hat.append((coeffs * past_vals).sum())\n",
    "    x_hat = torch.stack(x_hat)\n",
    "    \n",
    "    # Autoregressive residual loss\n",
    "    y_true = x_torch[p:]\n",
    "    loss_ar = torch.mean((y_true - x_hat) ** 2)\n",
    "    \n",
    "    # Sliding‑window power of the predicted signal\n",
    "    window_powers = []\n",
    "    for j in range(M):\n",
    "        start = max(0, j - W + 1)\n",
    "        window = x_hat[start:j+1]\n",
    "        power_hat = torch.mean(window**2)\n",
    "        window_powers.append(power_hat)\n",
    "    window_powers = torch.stack(window_powers)\n",
    "    loss_energy = torch.mean((window_powers - P0) ** 2)\n",
    "    \n",
    "    # Smoothness penalty on coefficient differences\n",
    "    diffs = coeff_pred_use[1:] - coeff_pred_use[:-1]\n",
    "    loss_smooth = torch.mean(torch.sum(diffs**2, dim=1))\n",
    "    \n",
    "    # Total loss\n",
    "    loss = loss_ar + lambda_energy * loss_energy + lambda_smooth * loss_smooth\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % (n_epochs // 10) == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, loss={loss.item():.6f}, ar={loss_ar.item():.6f}, energy={loss_energy.item():.6f}, smooth={loss_smooth.item():.6f}\")\n",
    "\n",
    "# After training, evaluate coefficients over all time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    coeff_pred_full = model(t_norm) * scale_coeff\n",
    "coeff_pred_full = coeff_pred_full.cpu().numpy()\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 7), sharex=True)\n",
    "axes[0].plot(loss_history)\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training loss')\n",
    "\n",
    "axes[1].plot(coeffs_true[:, 0], label='True a1')\n",
    "axes[1].plot(coeff_pred_full[:, 0], label='Predicted a1', linestyle='--')\n",
    "axes[1].set_ylabel('Coefficient a1')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(coeffs_true[:, 1], label='True a2')\n",
    "axes[2].plot(coeff_pred_full[:, 1], label='Predicted a2', linestyle='--')\n",
    "axes[2].set_ylabel('Coefficient a2')\n",
    "axes[2].set_xlabel('Time index')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c60f27",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "In this PyTorch implementation, automatic differentiation handles backpropagation through the network and the\n",
    "complex loss function, simplifying the coding effort compared to manual gradient calculations.\n",
    "The training loop may still take time to converge, and the estimates for the coefficients may not perfectly match\n",
    "the true trajectories due to identifiability issues and the weak power constraint.  You can experiment with\n",
    "different network sizes, loss weights (`lambda_energy`, `lambda_smooth`), learning rates, and epoch counts\n",
    "to improve performance.  Additionally, consider enforcing stability constraints on the AR coefficients or adding\n",
    "other physical constraints (e.g., spectral envelopes) to regularise the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638c860",
   "metadata": {},
   "source": [
    "## Automatic AR Order Selection with Gated PINN\n",
    "When the autoregressive order \\(p\\) is unknown, one approach is to fix a maximum order \\(p_{\\max}\\) and learn\n",
    "which lags are important using learnable gating weights.  Each lag \\(k\\) has a gate \\(g_k\\in[0,1]\\); the effective\n",
    "coefficient at time \\(t\\) is the product of the gate and the neural network’s output for that lag.  A sparsity\n",
    "penalty (L\\_1 regularisation) on the gates encourages unimportant lags to shrink towards zero.  After training,\n",
    "the number of gates above a threshold gives an estimated order \\(\\hat{p}\\).\n",
    "\n",
    "The code below defines a `TVARGatedPINN` class, trains it with a gating penalty, prints the learned gates,\n",
    "and uses them to reconstruct the signal and plot the results.  You can adjust `p_max`, `lambda_gates`, and\n",
    "other hyperparameters to suit your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deee4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gated PINN for automatic order selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Maximum AR order to consider\n",
    "p_max = 5  # adjust as needed\n",
    "\n",
    "# New network architecture producing p_max coefficients\n",
    "class TVARGatedPINN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=40, output_dim=p_max):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.tanh\n",
    "    def forward(self, t):\n",
    "        x = self.activation(self.fc1(t))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialise gating weights for each lag (size p_max); start with ones\n",
    "# We use nn.Parameter so gates are learnable\n",
    "gates = nn.Parameter(torch.ones(p_max, device=device))\n",
    "\n",
    "# Hyperparameters for the gated model\n",
    "hidden_dim_gated = 40\n",
    "scale_coeff_gated = 0.5\n",
    "lambda_energy_gated = 0.1\n",
    "lambda_smooth_gated = 0.01\n",
    "lambda_gates = 0.01  # sparsity penalty weight on gating weights\n",
    "learning_rate_gated = 0.01\n",
    "n_epochs_gated = 300\n",
    "\n",
    "# Instantiate model and optimizer (include gates in optimizer parameters)\n",
    "model_gated = TVARGatedPINN(input_dim=1, hidden_dim=hidden_dim_gated, output_dim=p_max).to(device)\n",
    "optimizer_gated = optim.Adam(list(model_gated.parameters()) + [gates], lr=learning_rate_gated)\n",
    "\n",
    "loss_history_gated = []\n",
    "for epoch in range(n_epochs_gated):\n",
    "    model_gated.train()\n",
    "    optimizer_gated.zero_grad()\n",
    "    \n",
    "    # Predict coefficients for all time points and apply scaling\n",
    "    coeff_pred = model_gated(t_norm) * scale_coeff_gated  # shape (N, p_max)\n",
    "    # Apply gates (broadcast multiply across time dimension)\n",
    "    coeff_eff = coeff_pred * gates  # shape (N, p_max)\n",
    "    \n",
    "    # Compute one-step predictions for each time >= 1\n",
    "    M = N - 1\n",
    "    x_hat = []\n",
    "    for i in range(1, N):\n",
    "        # Determine how many past lags we can use (up to p_max)\n",
    "        past_vals = []\n",
    "        for k in range(1, p_max + 1):\n",
    "            if i - k >= 0:\n",
    "                past_vals.append(x_torch[i - k])\n",
    "            else:\n",
    "                past_vals.append(torch.tensor(0.0, device=device))\n",
    "        past_vals = torch.stack(past_vals)\n",
    "        coeffs = coeff_eff[i]\n",
    "        x_hat.append((coeffs * past_vals).sum())\n",
    "    x_hat = torch.stack(x_hat)\n",
    "    \n",
    "    # True values\n",
    "    y_true = x_torch[1:]\n",
    "    # Loss components\n",
    "    loss_ar = torch.mean((y_true - x_hat) ** 2)\n",
    "    # Energy penalty computed on x_hat (sliding window)\n",
    "    window_powers = []\n",
    "    for j in range(M):\n",
    "        start = max(0, j - W + 1)\n",
    "        window = x_hat[start:j+1]\n",
    "        power_hat = torch.mean(window**2)\n",
    "        window_powers.append(power_hat)\n",
    "    window_powers = torch.stack(window_powers)\n",
    "    loss_energy = torch.mean((window_powers - P0) ** 2)\n",
    "    # Smoothness penalty on predicted coefficients\n",
    "    diffs = coeff_eff[1:] - coeff_eff[:-1]\n",
    "    loss_smooth = torch.mean(torch.sum(diffs ** 2, dim=1))\n",
    "    # Sparsity penalty on gates\n",
    "    loss_gates = torch.sum(torch.abs(gates))\n",
    "    # Total loss\n",
    "    loss_total = loss_ar + lambda_energy_gated * loss_energy + lambda_smooth_gated * loss_smooth + lambda_gates * loss_gates\n",
    "    loss_total.backward()\n",
    "    optimizer_gated.step()\n",
    "    loss_history_gated.append(loss_total.item())\n",
    "    \n",
    "    if (epoch + 1) % (n_epochs_gated // 5) == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs_gated}, loss={loss_total.item():.6f}\")\n",
    "\n",
    "# After training, examine the learned gates\n",
    "with torch.no_grad():\n",
    "    learned_gates = gates.cpu().numpy()\n",
    "\n",
    "print(\"Learned gating weights:\", learned_gates)\n",
    "# Estimate p by counting gates above a threshold (e.g., 0.1)\n",
    "estimated_p = int((learned_gates > 0.1).sum())\n",
    "print(f\"Estimated AR order (p) ≈ {estimated_p}\")\n",
    "\n",
    "# Use the gated coefficients to compute the predicted signal\n",
    "with torch.no_grad():\n",
    "    coeff_pred_full_gated = model_gated(t_norm) * scale_coeff_gated\n",
    "    coeff_eff_full = coeff_pred_full_gated * learned_gates\n",
    "coeff_eff_full = coeff_eff_full.cpu().numpy()\n",
    "\n",
    "# Construct predicted signal using the effective coefficients and observed past samples\n",
    "pred_signal_gated = np.zeros(N)\n",
    "pred_signal_gated[:p_max] = x[:p_max]\n",
    "for i in range(p_max, N):\n",
    "    past_vals = x[i-p_max:i][::-1]  # most recent first\n",
    "    pred_signal_gated[i] = np.dot(coeff_eff_full[i][:p_max], past_vals)\n",
    "\n",
    "# Plot true vs gated predicted signal\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, label='True signal')\n",
    "plt.plot(pred_signal_gated, label='Gated predicted signal', alpha=0.7)\n",
    "plt.title('True vs Gated Predicted Signal')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute lag embedding using the estimated tau and embedding dimension 3 as in previous section\n",
    "max_lag = 100\n",
    "acf = np.zeros(max_lag)\n",
    "for lag in range(1, max_lag):\n",
    "    if N - lag > 1:\n",
    "        corr = np.corrcoef(x[:-lag], x[lag:])[0, 1]\n",
    "    else:\n",
    "        corr = 0\n",
    "    acf[lag] = corr\n",
    "embedding_tau_gated = 1\n",
    "for i in range(1, max_lag - 1):\n",
    "    if acf[i] < acf[i - 1] and acf[i] < acf[i + 1]:\n",
    "        embedding_tau_gated = i\n",
    "        break\n",
    "embedding_dim = 3\n",
    "max_index = N - (embedding_dim - 1) * embedding_tau_gated\n",
    "embedding_true = np.column_stack([\n",
    "    x[j: j + max_index] for j in range(0, embedding_dim * embedding_tau_gated, embedding_tau_gated)\n",
    "])\n",
    "embedding_pred_gated = np.column_stack([\n",
    "    pred_signal_gated[j: j + max_index] for j in range(0, embedding_dim * embedding_tau_gated, embedding_tau_gated)\n",
    "])\n",
    "\n",
    "# Plot 3D phase portrait for gated prediction\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "idx = np.arange(0, embedding_true.shape[0], 5)\n",
    "ax.plot(embedding_true[idx, 0], embedding_true[idx, 1], embedding_true[idx, 2], label='True embedding', alpha=0.7)\n",
    "ax.plot(embedding_pred_gated[idx, 0], embedding_pred_gated[idx, 1], embedding_pred_gated[idx, 2], label='Gated predicted embedding', alpha=0.7)\n",
    "ax.set_title(f'Gated 3D Phase Portrait (tau = {embedding_tau_gated})')\n",
    "ax.set_xlabel('x(t)')\n",
    "ax.set_ylabel(f'x(t+{embedding_tau_gated})')\n",
    "ax.set_zlabel(f'x(t+2*{embedding_tau_gated})')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a45462",
   "metadata": {},
   "source": [
    "## Coefficients Comparison (Gated PINN)\n",
    "This cell compares the true time-varying AR coefficients with the effective coefficients learned by the gated PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96054e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the number of true AR coefficients\n",
    "p_true = coeffs_true.shape[1]\n",
    "# Plot true and gated predicted coefficients for each lag up to p_true\n",
    "plt.figure(figsize=(10, 4))\n",
    "for k in range(p_true):\n",
    "    plt.plot(coeffs_true[:, k], label=f'True a{k+1}')\n",
    "    plt.plot(coeff_eff_full[:, k], label=f'Gated predicted a{k+1}', linestyle='--')\n",
    "plt.title('True vs Gated Predicted AR Coefficients')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
