{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 224\u001b[0m\n\u001b[1;32m    222\u001b[0m N_te \u001b[38;5;241m=\u001b[39m N \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    223\u001b[0m y_true \u001b[38;5;241m=\u001b[39m x[t0 : t0 \u001b[38;5;241m+\u001b[39m N_te]\n\u001b[0;32m--> 224\u001b[0m yhat_ctrl \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREFRESH_EVERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAG_MAX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# ------------- (3) Test-window prediction -------------\u001b[39;00m\n\u001b[1;32m    227\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m, in \u001b[0;36mhybrid_rollout\u001b[0;34m(model, series_norm, start_idx, n_steps, refresh_every, L)\u001b[0m\n\u001b[1;32m    207\u001b[0m t_abs \u001b[38;5;241m=\u001b[39m start_idx \u001b[38;5;241m+\u001b[39m t\n\u001b[1;32m    208\u001b[0m x_slice \u001b[38;5;241m=\u001b[39m cur_t[:, :t_abs]  \u001b[38;5;66;03m# [1, t_abs]\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m mu, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_slice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m xhat_t \u001b[38;5;241m=\u001b[39m mu[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    211\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(xhat_t)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m, in \u001b[0;36mLagAttentionTVAR.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# per-time encode lag tokens\u001b[39;00m\n\u001b[1;32m    141\u001b[0m Hlag_flat \u001b[38;5;241m=\u001b[39m Hlag\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m Henc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlag_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHlag_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(B, T, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B,T,L,d]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# causal context from x\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx_conv(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))        \u001b[38;5;66;03m# [B,d,T+pad]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:871\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[1;32m    868\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(\n\u001b[1;32m    869\u001b[0m             src_mask, src_key_padding_mask, src\n\u001b[1;32m    870\u001b[0m         )\n\u001b[0;32m--> 871\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\u001b[39;00m\n\u001b[1;32m    895\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lag-Attention tvAR pipeline on Lorenz (publication-style, one chart per figure, no explicit colors)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------- Global plot style -------------\n",
    "def set_pub_style():\n",
    "    plt.rcParams.update({\n",
    "        \"figure.dpi\": 120,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"lines.linewidth\": 1.8,\n",
    "        \"lines.markersize\": 5,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.25,\n",
    "    })\n",
    "\n",
    "def prettify(ax, title=None, xlabel=None, ylabel=None, add_legend=False):\n",
    "    if title: ax.set_title(title)\n",
    "    if xlabel: ax.set_xlabel(xlabel)\n",
    "    if ylabel: ax.set_ylabel(ylabel)\n",
    "    if add_legend: ax.legend(frameon=False)\n",
    "    ax.margins(x=0.02)\n",
    "\n",
    "set_pub_style()\n",
    "\n",
    "\n",
    "# ------------- Lorenz (RK4) -------------\n",
    "def lorenz(T_steps=300, dt=0.005, sigma=10.0, rho=28.0, beta=8/3, x0=(1.0,1.0,1.0)):\n",
    "    x, y, z = x0\n",
    "    xs, ys, zs = [], [], []\n",
    "    for _ in range(T_steps):\n",
    "        def f(x,y,z):\n",
    "            dx = sigma*(y-x)\n",
    "            dy = x*(rho - z) - y\n",
    "            dz = x*y - beta*z\n",
    "            return dx, dy, dz\n",
    "        k1 = f(x,y,z)\n",
    "        k2 = f(x + 0.5*dt*k1[0], y + 0.5*dt*k1[1], z + 0.5*dt*k1[2])\n",
    "        k3 = f(x + 0.5*dt*k2[0], y + 0.5*dt*k2[1], z + 0.5*dt*k2[2])\n",
    "        k4 = f(x + dt*k3[0], y + dt*k3[1], z + dt*k3[2])\n",
    "        x += (dt/6.0)*(k1[0] + 2*k2[0] + 2*k3[0] + k4[0])\n",
    "        y += (dt/6.0)*(k1[1] + 2*k2[1] + 2*k3[1] + k4[1])\n",
    "        z += (dt/6.0)*(k1[2] + 2*k2[2] + 2*k3[2])\n",
    "        xs.append(x); ys.append(y); zs.append(z)\n",
    "    return np.array(xs), np.array(ys), np.array(zs)\n",
    "\n",
    "xs, ys, zs = lorenz()\n",
    "\n",
    "# ------------- Helpers -------------\n",
    "def acf_vals(sig, nlxags=60):\n",
    "    s = sig - np.mean(sig)\n",
    "    ac = np.correlate(s, s, mode=\"full\")\n",
    "    ac = ac[ac.size//2:]\n",
    "    ac0 = ac[0] if ac[0] != 0 else 1.0\n",
    "    return (ac / ac0)[:nlags+1]\n",
    "\n",
    "def embed3(s, tau):\n",
    "    # returns [N-2*tau, 3]: [s(t), s(t-τ), s(t-2τ)]\n",
    "    return np.column_stack([s[2*tau:], s[tau:-tau], s[:-2*tau]])\n",
    "\n",
    "# ------------- Train / test split on x(t) -------------\n",
    "LAG_MAX   = 256     # number of candidate lags (L)\n",
    "TRAIN_FR  = 0.80    # 80/20 split\n",
    "REFRESH_EVERY = 20  # controllable rollout knob: 1 => 1-step ; large => free-run\n",
    "TAU = 15           # state-space embedding delay (samples)\n",
    "\n",
    "x = xs.copy()\n",
    "N = len(x)\n",
    "ntr = int(TRAIN_FR * (N - LAG_MAX))  # leave room for left padding by L\n",
    "# scale by train stats\n",
    "mu_tr  = x[:LAG_MAX + ntr].mean()\n",
    "std_tr = x[:LAG_MAX + ntr].std() + 1e-12\n",
    "x_n = (x - mu_tr) / std_tr\n",
    "\n",
    "# ------------- Lag-attention tvAR (univariate) -------------\n",
    "def build_lag_bank(x_tensor, L):\n",
    "    # x_tensor: [B, T], returns [B, T, L] with Xlags[:, t, l] = x_{t-(l+1)}\n",
    "    B, T = x_tensor.shape\n",
    "    pads = F.pad(x_tensor, (L, 0))\n",
    "    lags = [pads[:, L - (l+1) : L - (l+1) + T] for l in range(L)]\n",
    "    return torch.stack(lags, dim=-1)\n",
    "\n",
    "def topk_mask_logits(logits, k):\n",
    "    if (k is None) or (k >= logits.size(-1)):\n",
    "        return logits\n",
    "    topk = torch.topk(logits, k, dim=-1)\n",
    "    mask = torch.full_like(logits, float('-inf'))\n",
    "    mask.scatter_(-1, topk.indices, topk.values)\n",
    "    return mask\n",
    "\n",
    "class LagAttentionTVAR(nn.Module):\n",
    "    def __init__(self, L=256, d_model=128, n_layers=2, n_heads=4, topk=8, use_var=True):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.topk = topk\n",
    "        self.use_var = use_var\n",
    "\n",
    "        self.lag_embed = nn.Embedding(L+1, d_model)  # lag index 1..L\n",
    "        self.val_proj  = nn.Linear(1, d_model)\n",
    "\n",
    "        # small causal conv context\n",
    "        self.ctx_conv = nn.Conv1d(1, d_model, kernel_size=9, padding=8)\n",
    "        self.ctx_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.lag_encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Linear(d_model*2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.bias_head   = nn.Linear(d_model, 1)\n",
    "        if use_var:\n",
    "            self.logvar_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        B, T = x.shape\n",
    "        L = self.L\n",
    "\n",
    "        Xlags = build_lag_bank(x, L)               # [B,T,L]\n",
    "        lag_vals = Xlags.unsqueeze(-1)             # [B,T,L,1]\n",
    "        lag_ids  = torch.arange(1, L+1, device=x.device).view(1,1,L).expand(B,T,L)\n",
    "\n",
    "        Hval = self.val_proj(lag_vals).squeeze(-2) # [B,T,L,d]\n",
    "        Hidx = self.lag_embed(lag_ids)             # [B,T,L,d]\n",
    "        Hlag = Hval + Hidx                         # [B,T,L,d]\n",
    "\n",
    "        # per-time encode lag tokens\n",
    "        Hlag_flat = Hlag.view(B*T, L, -1)\n",
    "        Henc = self.lag_encoder(Hlag_flat).view(B, T, L, -1)  # [B,T,L,d]\n",
    "\n",
    "        # causal context from x\n",
    "        ctx = self.ctx_conv(x.unsqueeze(1))        # [B,d,T+pad]\n",
    "        ctx = ctx[..., :T].transpose(1,2)          # [B,T,d]\n",
    "        ctx = self.ctx_proj(ctx)                   # [B,T,d]\n",
    "\n",
    "        # score lags with context\n",
    "        ctx_exp = ctx.unsqueeze(2).expand(-1, -1, L, -1)\n",
    "        pair = torch.cat([Henc, ctx_exp], dim=-1)  # [B,T,L,2d]\n",
    "        logits = self.score(pair).squeeze(-1)      # [B,T,L]\n",
    "\n",
    "        logits_masked = topk_mask_logits(logits, k=self.topk)\n",
    "        w = torch.softmax(logits_masked, dim=-1)   # [B,T,L], sums to 1\n",
    "\n",
    "        mu_ar = (w * Xlags).sum(dim=-1)            # [B,T]\n",
    "        c  = self.bias_head(ctx).squeeze(-1)       # [B,T]\n",
    "        mu = mu_ar + c\n",
    "\n",
    "        logvar = None\n",
    "        if self.use_var:\n",
    "            logvar = self.logvar_head(ctx).squeeze(-1).clamp(-8, 8)\n",
    "        return mu, logvar, w\n",
    "\n",
    "def gaussian_nll(y, mu, logvar):\n",
    "    if logvar is None:\n",
    "        logvar = torch.zeros_like(mu)\n",
    "    return 0.5*(logvar + (y - mu)**2 / logvar.exp())\n",
    "\n",
    "# ------------- Train tvAR on train split (teacher-forced 1-step) -------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "model = LagAttentionTVAR(L=LAG_MAX, d_model=128, n_layers=2, n_heads=4, topk=8, use_var=False).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "\n",
    "# use contiguous block up to train end; keep last part for test\n",
    "x_tr = torch.tensor(x_n[:LAG_MAX + ntr], dtype=torch.float32, device=device).unsqueeze(0)  # [1, Ttr]\n",
    "EPOCHS = 10  # keep small for demo; increase for better fit\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    mu, logvar, w = model(x_tr)\n",
    "    # predict current index; you can shift by 1 if preferred—here Xlags already uses past values\n",
    "    nll = gaussian_nll(x_tr, mu, logvar).mean()\n",
    "    # mild sparsity on weights\n",
    "    l1 = w.abs().mean()\n",
    "    loss = nll + 1e-3 * l1\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# ------------- Controlled rollout on test (refresh-every-k) -------------\n",
    "def hybrid_rollout(model, series_norm, start_idx, n_steps, refresh_every, L):\n",
    "    \"\"\"\n",
    "    series_norm: normalized full series (numpy)\n",
    "    start_idx: absolute start index in series_norm\n",
    "    returns denormalized predictions over test window\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cur = series_norm.copy()\n",
    "    cur_t = torch.tensor(cur, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        # we will iterate step-by-step to respect refresh schedule\n",
    "        for t in range(n_steps):\n",
    "            # take a slice up to current absolute time (inclusive) for causality\n",
    "            t_abs = start_idx + t\n",
    "            x_slice = cur_t[:, :t_abs]  # [1, t_abs]\n",
    "            mu, _, _ = model(x_slice)\n",
    "            xhat_t = mu[0, -1].item()\n",
    "            preds.append(xhat_t)\n",
    "            if ((t+1) % max(int(refresh_every),1)) != 0:\n",
    "                # open-loop: append prediction into working series\n",
    "                cur[t_abs] = xhat_t\n",
    "                cur_t[0, t_abs] = xhat_t\n",
    "            # else: refresh with truth implicitly by leaving series as-is\n",
    "    preds = np.array(preds)\n",
    "    return preds * std_tr + mu_tr  # de-normalize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/dq7c13p16bsb4qhmx8fvgp2m0000gn/T/ipykernel_32723/1245170986.py:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
      "/var/folders/zl/dq7c13p16bsb4qhmx8fvgp2m0000gn/T/ipykernel_32723/1245170986.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2707 is out of bounds for dimension 0 with size 2707",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m(device\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     mu, logvar, w \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# teacher forced; predicts each t from its past\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# drop warm-up LAG_MAX steps from loss (left padding period)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     nll \u001b[38;5;241m=\u001b[39m gaussian_nll(x_tr[:, LAG_MAX:], mu[:, LAG_MAX:], logvar)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mLagAttentionTVARFast.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# build lag bank (past-only)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m Xlags \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_lag_bank_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m                \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m lag_vals \u001b[38;5;241m=\u001b[39m Xlags\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                   \u001b[38;5;66;03m# [B,T,L,1]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m lag_ids  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, L\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,L)\u001b[38;5;241m.\u001b[39mexpand(B,T,L)\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mbuild_lag_bank_fast\u001b[0;34m(x_tensor, L)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# shift by offset (L-1) so the last column corresponds to x_{t-1}\u001b[39;00m\n\u001b[1;32m     20\u001b[0m gather_idx \u001b[38;5;241m=\u001b[39m L \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m gather_idx                \u001b[38;5;66;03m# [1,T,L]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m Xlags \u001b[38;5;241m=\u001b[39m \u001b[43mpads\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m         \u001b[38;5;66;03m# [B,T,L]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xlags\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2707 is out of bounds for dimension 0 with size 2707"
     ]
    }
   ],
   "source": [
    "# ======== FAST Lag-Attention tvAR (no Transformer) ========\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_lag_bank_fast(x_tensor, L):\n",
    "    \"\"\"\n",
    "    x_tensor: [B, T]\n",
    "    Returns Xlags[B, T, L] with Xlags[:, t, l] = x_{t-(l+1)} (left-padded with zeros).\n",
    "    Vectorized (no Python loops over L).\n",
    "    \"\"\"\n",
    "    B, T = x_tensor.shape\n",
    "    pads = F.pad(x_tensor, (L, 0))                 # [B, T+L]\n",
    "    # Build indices so that for each t we gather T rows that start at offset (t .. t+L-1)\n",
    "    idx_base = torch.arange(T, device=x_tensor.device).view(1, T, 1)  # [1,T,1]\n",
    "    lag_offsets = torch.arange(L, device=x_tensor.device).view(1, 1, L)  # [1,1,L]\n",
    "    gather_idx = idx_base + lag_offsets            # [1,T,L]\n",
    "    # shift by offset (L-1) so the last column corresponds to x_{t-1}\n",
    "    gather_idx = L - 1 + gather_idx                # [1,T,L]\n",
    "    Xlags = pads[:, gather_idx.squeeze(0)]         # [B,T,L]\n",
    "    return Xlags\n",
    "\n",
    "class LagAttentionTVARFast(nn.Module):\n",
    "    def __init__(self, L=256, d_model=128, topk=8, use_var=False):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.topk = topk\n",
    "        self.use_var = use_var\n",
    "\n",
    "        # embeddings for lag indices (1..L) and projected lag values\n",
    "        self.lag_embed = nn.Embedding(L+1, d_model)     # id 1..L\n",
    "        self.val_proj  = nn.Linear(1, d_model)\n",
    "\n",
    "        # strictly causal context (left pad only)\n",
    "        self.ctx_pad  = nn.ConstantPad1d((8, 0), 0)     # pad left for k=9\n",
    "        self.ctx_conv = nn.Conv1d(1, d_model, kernel_size=9, padding=0)\n",
    "        self.ctx_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # bilinear scorer: score_{t,ℓ} = < Wq * ctx_t , Wk * Hlag_{t,ℓ} >\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # bias term from context\n",
    "        self.bias_head = nn.Linear(d_model, 1)\n",
    "        if use_var:\n",
    "            self.logvar_head = nn.Linear(d_model, 1)\n",
    "\n",
    "        # init a bit smaller for stability\n",
    "        for m in [self.Wq, self.Wk, self.val_proj, self.bias_head]:\n",
    "            if hasattr(m, 'weight'):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T]\n",
    "        returns: mu[B,T], logvar[B,T] or None, w[B,T,L]\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        L = self.L\n",
    "\n",
    "        # build lag bank (past-only)\n",
    "        Xlags = build_lag_bank_fast(x, L)                # [B,T,L]\n",
    "        lag_vals = Xlags.unsqueeze(-1)                   # [B,T,L,1]\n",
    "        lag_ids  = torch.arange(1, L+1, device=x.device).view(1,1,L).expand(B,T,L)\n",
    "\n",
    "        Hval = self.val_proj(lag_vals).squeeze(-2)       # [B,T,L,d]\n",
    "        Hidx = self.lag_embed(lag_ids)                   # [B,T,L,d]\n",
    "        Hlag = Hval + Hidx                               # [B,T,L,d]\n",
    "\n",
    "        # causal context from x\n",
    "        ctx = self.ctx_conv(self.ctx_pad(x.unsqueeze(1)))   # [B,d,T]\n",
    "        ctx = ctx.transpose(1, 2)                            # [B,T,d]\n",
    "        ctx = self.ctx_proj(ctx)                             # [B,T,d]\n",
    "\n",
    "        # bilinear scores\n",
    "        q = self.Wq(ctx)                                     # [B,T,d]\n",
    "        k = self.Wk(Hlag)                                    # [B,T,L,d]\n",
    "        # score_{t,ℓ} = sum_d q_{t,d} * k_{t,ℓ,d}\n",
    "        logits = torch.einsum('btd,btld->btl', q, k)         # [B,T,L]\n",
    "\n",
    "        # optional top-k masking for sparsity/speed in softmax\n",
    "        if (self.topk is not None) and (self.topk < L):\n",
    "            topk = torch.topk(logits, self.topk, dim=-1)\n",
    "            mask = torch.full_like(logits, float('-inf'))\n",
    "            logits = mask.scatter(-1, topk.indices, topk.values)\n",
    "\n",
    "        w = torch.softmax(logits, dim=-1)                    # [B,T,L], sums to 1\n",
    "        mu_ar = (w * Xlags).sum(dim=-1)                      # [B,T]\n",
    "        c = self.bias_head(ctx).squeeze(-1)                  # [B,T]\n",
    "        mu = mu_ar + c\n",
    "\n",
    "        logvar = None\n",
    "        if self.use_var:\n",
    "            logvar = self.logvar_head(ctx).squeeze(-1).clamp(-8, 8)\n",
    "        return mu, logvar, w\n",
    "\n",
    "def gaussian_nll(y, mu, logvar):\n",
    "    if logvar is None:\n",
    "        logvar = torch.zeros_like(mu)\n",
    "    return 0.5*(logvar + (y - mu)**2 / (logvar.exp() + 1e-8))\n",
    "\n",
    "# ======== TRAINING (with prints) ========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = LagAttentionTVARFast(L=LAG_MAX, d_model=128, topk=8, use_var=False).to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "x_tr = torch.tensor(x_n[:LAG_MAX + ntr], dtype=torch.float32, device=device).unsqueeze(0)  # [1,Ttr]\n",
    "x_val = torch.tensor(x_n[LAG_MAX + ntr - 2048 : LAG_MAX + ntr], dtype=torch.float32, device=device).unsqueeze(0)  # last 2048 as val context\n",
    "\n",
    "EPOCHS = 20\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
    "        mu, logvar, w = model(x_tr)  # teacher forced; predicts each t from its past\n",
    "        # drop warm-up LAG_MAX steps from loss (left padding period)\n",
    "        nll = gaussian_nll(x_tr[:, LAG_MAX:], mu[:, LAG_MAX:], logvar).mean()\n",
    "        l1  = w.abs().mean()\n",
    "        loss = nll + 1e-3 * l1\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(opt)\n",
    "    scaler.update()\n",
    "\n",
    "    # quick val (no grad)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_v, lv_v, w_v = model(x_val)\n",
    "        val_loss = gaussian_nll(x_val[:, LAG_MAX:], mu_v[:, LAG_MAX:], lv_v).mean().item()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    if ep % PRINT_EVERY == 0:\n",
    "        print(f\"[Epoch {ep:02d}] train_loss={loss.item():.5f}  val_loss={val_loss:.5f}  |T|={x_tr.shape[1]}  L={model.L}  time={dt:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for test region (use last 20% after L left pad)\n",
    "t0 = LAG_MAX + ntr\n",
    "N_te = N - t0\n",
    "y_true = x[t0 : t0 + N_te]\n",
    "yhat_ctrl = hybrid_rollout(model, x_n, start_idx=t0, n_steps=N_te, refresh_every=REFRESH_EVERY, L=LAG_MAX)\n",
    "\n",
    "# ------------- (3) Test-window prediction -------------\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.plot(y_true, lw=1.6, label=\"Truth\")\n",
    "ax.plot(yhat_ctrl, lw=1.4, label=f\"tvAR (refresh={REFRESH_EVERY})\")\n",
    "prettify(ax, title=\"Lag-Attention tvAR — test window\", xlabel=\"Time (test index)\", ylabel=\"x(t)\", add_legend=True)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------- (4) Residual ACF (test) -------------\n",
    "resid = y_true - yhat_ctrl\n",
    "acf_v = acf_vals(resid, nlags=60)\n",
    "bound = 1.96 / np.sqrt(max(len(resid),1))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "markerline, stemlines, baseline = ax.stem(range(len(acf_v)), acf_v, basefmt=\" \")\n",
    "plt.setp(stemlines, linewidth=1.2)\n",
    "plt.setp(markerline, marker='o', markersize=5)\n",
    "ax.axhline(bound, linestyle='--', linewidth=1)\n",
    "ax.axhline(-bound, linestyle='--', linewidth=1)\n",
    "prettify(ax, title=\"Residual ACF — tvAR on test\", xlabel=\"Lag\", ylabel=\"Autocorrelation\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------- (5) y_pred vs y_true (test) -------------\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax.scatter(y_true, yhat_ctrl, alpha=0.35, s=10, edgecolors=\"none\")\n",
    "vmin = min(np.min(y_true), np.min(yhat_ctrl))\n",
    "vmax = max(np.max(y_true), np.max(yhat_ctrl))\n",
    "ax.plot([vmin, vmax], [vmin, vmax], lw=1)\n",
    "yt = y_true - np.mean(y_true)\n",
    "yp = yhat_ctrl - np.mean(yhat_ctrl)\n",
    "corr = float((yt @ yp) / np.sqrt((yt @ yt) * (yp @ yp) + 1e-12))\n",
    "ss_res = np.sum((y_true - yhat_ctrl)**2)\n",
    "ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "r2 = 1 - ss_res / (ss_tot + 1e-12)\n",
    "ax.text(0.05, 0.95, f\"$R^2$ = {r2:.3f}\\n$\\\\rho$ = {corr:.3f}\",\n",
    "        transform=ax.transAxes, ha='left', va='top',\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7, boxstyle=\"round\"))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "prettify(ax, title=\"Test scatter: y_pred vs y_true\", xlabel=\"y_true\", ylabel=\"y_pred\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------- (6) State space — TRUE vs tvAR (variance-matched) -------------\n",
    "# variance-match for fair geometry comparison\n",
    "hy = (yhat_ctrl - yhat_ctrl.mean())\n",
    "hy = hy * (y_true.std() / (hy.std() + 1e-12)) + y_true.mean()\n",
    "X3_true = embed3(y_true, TAU)\n",
    "X3_hyb  = embed3(hy,      TAU)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "fig = plt.figure(figsize=(7.5, 5.5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(X3_true[:,0], X3_true[:,1], X3_true[:,2], label=\"TRUE\", alpha=0.85)\n",
    "ax.plot(X3_hyb[:,0],  X3_hyb[:,1],  X3_hyb[:,2],  label=f\"tvAR (refresh={REFRESH_EVERY})\", alpha=0.85)\n",
    "ax.set_title(f\"State space — TRUE vs tvAR (τ={TAU}, refresh={REFRESH_EVERY})\")\n",
    "ax.set_xlabel(\"x(t)\"); ax.set_ylabel(f\"x(t-{TAU})\"); ax.set_zlabel(f\"x(t-{2*TAU})\")\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------- (7) Lag-weight heatmap over test window -------------\n",
    "# recompute weights over test with teacher-forcing (to visualize selection cleanly)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test_tf = torch.tensor(x_n[:t0+N_te], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    _, _, w_full = model(x_test_tf)             # [1, T, L]\n",
    "    w_test = w_full[0, t0:, :].cpu().numpy()    # [N_te, L]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "# show newest lags on the left if you prefer: w_test[:, :]; here index 0 = lag-1\n",
    "im = ax.imshow(w_test.T, aspect='auto', origin='lower', interpolation='nearest')\n",
    "ax.set_ylabel(\"Lag index (ℓ = 1..L)\")\n",
    "ax.set_xlabel(\"Time (test index)\")\n",
    "ax.set_title(\"Lag-attention weights w_ℓ(t) on test\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
