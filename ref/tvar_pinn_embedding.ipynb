{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c46b573",
   "metadata": {},
   "source": [
    "# TVAR PINN: Lag Embedding and 3D Phase Portrait\n",
    "This notebook expands upon the physics-informed neural network (PINN) demonstration for a time-varying autoregressive\n",
    "model with constant sliding-window power.  In addition to simulating and training the PINN, it computes a lag embedding\n",
    "of the observed and predicted signals and displays a 3D phase portrait.  The code is organised into separate sections\n",
    "for simulation, PINN definition, training, estimation, and analysis so that the PINN can be reused on real data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab988e8",
   "metadata": {},
   "source": [
    "## Background\n",
    "Time-varying autoregressive (TVAR) models describe signals whose autoregressive coefficients change slowly over time.\n",
    "Estimating these coefficients is challenging: multiple coefficient trajectories can reproduce a signal with similar\n",
    "energy, so the problem is often ill-posed【988150528581547†L918-L933】.  A physics-informed neural network embeds structural\n",
    "constraints—here, constant sliding-window power and smooth coefficient variations—into the loss function to improve\n",
    "identifiability【672647501557241†L152-L160】.  This notebook simulates a TVAR process with low-frequency oscillatory dynamics\n",
    "similar to local field potentials (LFPs), trains a simple PINN to estimate the time-varying coefficients, and then\n",
    "reconstructs the predicted signal.  Finally, it computes a delay embedding (lag embedding) and shows the 3D phase\n",
    "portrait of both the observed and predicted signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eda6c",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "The following function simulates a TVAR process of order `p` with time-dependent coefficients.\n",
    "The signal is scaled within a sliding window of length `W` so that its average squared amplitude (power)\n",
    "remains close to a target value `P0`.  Gaussian noise adds variability.  The function returns the scaled\n",
    "signal and the true time-varying coefficients as arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def simulate_tvar_constant_power(N: int, p: int, W: int, P0: float,\n",
    "                                coeff_funcs, noise_std: float = 0.1,\n",
    "                                seed: int = 0, c_min: float | None = None,\n",
    "                                c_max: float | None = None):\n",
    "    # Generate a length-N TVAR(p) sequence with constant sliding-window power\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x_raw = np.zeros(N)\n",
    "    x_scaled = np.zeros(N)\n",
    "    coeffs_true = np.zeros((N, p))\n",
    "    for t in range(N):\n",
    "        coeffs_true[t] = [func(t) for func in coeff_funcs]\n",
    "    S_prev = 0.0\n",
    "    for t in range(N):\n",
    "        val = rng.normal(scale=noise_std)\n",
    "        for k in range(1, p + 1):\n",
    "            if t >= k:\n",
    "                val += coeffs_true[t, k - 1] * x_scaled[t - k]\n",
    "        x_raw[t] = val\n",
    "        if t == 0:\n",
    "            S_prev = 0.0\n",
    "        else:\n",
    "            if t < W:\n",
    "                S_prev = np.sum(x_scaled[:t] ** 2)\n",
    "            else:\n",
    "                S_prev = S_prev + x_scaled[t - 1] ** 2 - x_scaled[t - W] ** 2\n",
    "        if t >= W - 1:\n",
    "            target_energy = P0 * W\n",
    "            denom = x_raw[t] ** 2 + 1e-12\n",
    "            c_sq = max((target_energy - S_prev) / denom, 0.0)\n",
    "            c = np.sqrt(c_sq)\n",
    "        else:\n",
    "            c = 1.0\n",
    "        if c_min is not None or c_max is not None:\n",
    "            if c_min is None:\n",
    "                c_min = -np.inf\n",
    "            if c_max is None:\n",
    "                c_max = np.inf\n",
    "            c = np.clip(c, c_min, c_max)\n",
    "        x_scaled[t] = c * x_raw[t]\n",
    "    return x_scaled, coeffs_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b80b7",
   "metadata": {},
   "source": [
    "## PINN Definition and Training\n",
    "To estimate the time-varying AR coefficients, we use a simple two-hidden-layer neural network implemented\n",
    "with NumPy.  The network takes a normalised time input `t_norm` and outputs the coefficients at that time.\n",
    "The training function minimises a composite loss consisting of an autoregressive residual, an energy penalty\n",
    "that enforces constant sliding-window power of the reconstructed signal, and a smoothness penalty on\n",
    "coefficient variations.  These functions are modular so they can be reused for real data by replacing the\n",
    "simulated signal `x` with a real time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa49593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimplePINN:\n",
    "    def __init__(self, p: int, h1: int = 20, h2: int = 20, seed: int = 0):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = rng.normal(scale=0.5, size=(1, h1))\n",
    "        self.b1 = np.zeros(h1)\n",
    "        self.W2 = rng.normal(scale=0.5, size=(h1, h2))\n",
    "        self.b2 = np.zeros(h2)\n",
    "        self.W3 = rng.normal(scale=0.5, size=(h2, p))\n",
    "        self.b3 = np.zeros(p)\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    @staticmethod\n",
    "    def dtanh(x):\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "    def forward(self, t: np.ndarray):\n",
    "        z1 = t @ self.W1 + self.b1\n",
    "        h1 = self.tanh(z1)\n",
    "        z2 = h1 @ self.W2 + self.b2\n",
    "        h2 = self.tanh(z2)\n",
    "        out = h2 @ self.W3 + self.b3\n",
    "        cache = {\"t\": t, \"z1\": z1, \"h1\": h1, \"z2\": z2, \"h2\": h2}\n",
    "        return out, cache\n",
    "    def backward(self, cache: dict, dL_dout: np.ndarray):\n",
    "        z1, h1 = cache[\"z1\"], cache[\"h1\"]\n",
    "        z2, h2 = cache[\"z2\"], cache[\"h2\"]\n",
    "        t = cache[\"t\"]\n",
    "        M = dL_dout.shape[0]\n",
    "        dW3 = np.zeros_like(self.W3)\n",
    "        db3 = np.zeros_like(self.b3)\n",
    "        dW2 = np.zeros_like(self.W2)\n",
    "        db2 = np.zeros_like(self.b2)\n",
    "        dW1 = np.zeros_like(self.W1)\n",
    "        db1 = np.zeros_like(self.b1)\n",
    "        for i in range(M):\n",
    "            g_c = dL_dout[i]\n",
    "            h2_i = h2[i]\n",
    "            dW3 += np.outer(h2_i, g_c)\n",
    "            db3 += g_c\n",
    "            g_h2 = g_c @ self.W3.T\n",
    "            g_z2 = g_h2 * self.dtanh(z2[i])\n",
    "            h1_i = h1[i]\n",
    "            dW2 += np.outer(h1_i, g_z2)\n",
    "            db2 += g_z2\n",
    "            g_h1 = g_z2 @ self.W2.T\n",
    "            g_z1 = g_h1 * self.dtanh(z1[i])\n",
    "            t_i = t[i, 0]\n",
    "            dW1 += t_i * g_z1.reshape(1, -1)\n",
    "            db1 += g_z1\n",
    "        grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2, \"W3\": dW3, \"b3\": db3}\n",
    "        return grads\n",
    "    def step(self, grads: dict, lr: float):\n",
    "        self.W1 -= lr * grads[\"W1\"]\n",
    "        self.b1 -= lr * grads[\"b1\"]\n",
    "        self.W2 -= lr * grads[\"W2\"]\n",
    "        self.b2 -= lr * grads[\"b2\"]\n",
    "        self.W3 -= lr * grads[\"W3\"]\n",
    "        self.b3 -= lr * grads[\"b3\"]\n",
    "\n",
    "\n",
    "def train_pinn(x: np.ndarray, p: int, W: int, P0: float, n_epochs: int = 200,\n",
    "               lr: float = 0.01, lambda_energy: float = 0.1, lambda_smooth: float = 0.01,\n",
    "               h1: int = 20, h2: int = 20, seed: int = 0, scale_coeff: float = 0.5):\n",
    "    N = len(x)\n",
    "    t_vals = np.linspace(0.0, 1.0, N)\n",
    "    train_t = t_vals.reshape(-1, 1)\n",
    "    pinn = SimplePINN(p=p, h1=h1, h2=h2, seed=seed)\n",
    "    hist_total, hist_ar, hist_energy, hist_smooth = [], [], [], []\n",
    "    for epoch in range(n_epochs):\n",
    "        out, cache = pinn.forward(train_t)\n",
    "        coeffs_out = scale_coeff * out\n",
    "        residuals = np.zeros(N)\n",
    "        x_hat = np.zeros(N)\n",
    "        for i in range(p, N):\n",
    "            past_vals = np.array([x[i - k] for k in range(1, p + 1)])\n",
    "            coeffs = coeffs_out[i]\n",
    "            x_hat[i] = np.dot(coeffs, past_vals)\n",
    "            residuals[i] = x[i] - x_hat[i]\n",
    "        loss_ar = np.mean(residuals[p:] ** 2)\n",
    "        P_hat = np.zeros(N)\n",
    "        for i in range(p, N):\n",
    "            start = max(p, i - W + 1)\n",
    "            window = x_hat[start:i + 1]\n",
    "            P_hat[i] = np.mean(window ** 2) if window.size > 0 else 0.0\n",
    "        loss_energy = np.mean((P_hat[p:] - P0) ** 2)\n",
    "        diff = coeffs_out[1:] - coeffs_out[:-1]\n",
    "        loss_smooth = np.mean(np.sum(diff ** 2, axis=1))\n",
    "        loss_total = loss_ar + lambda_energy * loss_energy + lambda_smooth * loss_smooth\n",
    "        hist_total.append(loss_total)\n",
    "        hist_ar.append(loss_ar)\n",
    "        hist_energy.append(loss_energy)\n",
    "        hist_smooth.append(loss_smooth)\n",
    "        dL_dout = np.zeros_like(out)\n",
    "        for i in range(p, N):\n",
    "            past_vals = np.array([x[i - k] for k in range(1, p + 1)])\n",
    "            d = -2.0 * residuals[i] / (N - p)\n",
    "            dL_dout[i] += d * (scale_coeff * past_vals)\n",
    "        dL_dPhat = (2.0 / (N - p)) * (P_hat - P0)\n",
    "        dL_d_xhat = np.zeros(N)\n",
    "        for j in range(p, N):\n",
    "            i_start = j\n",
    "            i_end = min(N - 1, j + W - 1)\n",
    "            if i_end >= i_start:\n",
    "                s = np.sum(dL_dPhat[i_start:i_end + 1])\n",
    "                dL_d_xhat[j] = s * (2.0 / W) * x_hat[j]\n",
    "        for j in range(p, N):\n",
    "            past_vals = np.array([x[j - k] for k in range(1, p + 1)])\n",
    "            dL_dout[j] += lambda_energy * dL_d_xhat[j] * (scale_coeff * past_vals)\n",
    "        for k in range(1, N):\n",
    "            grad = np.zeros(p)\n",
    "            if k < N - 1:\n",
    "                grad += 2 * (coeffs_out[k] - coeffs_out[k + 1])\n",
    "            if k > 0:\n",
    "                grad += 2 * (coeffs_out[k] - coeffs_out[k - 1])\n",
    "            dL_dout[k] += (lambda_smooth / N) * grad\n",
    "        dL_dout *= scale_coeff\n",
    "        grads = pinn.backward(cache, dL_dout)\n",
    "        pinn.step(grads, lr)\n",
    "    history = {\"total\": hist_total, \"ar\": hist_ar, \"energy\": hist_energy, \"smooth\": hist_smooth}\n",
    "    return pinn, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70663c25",
   "metadata": {},
   "source": [
    "## Estimation and Analysis\n",
    "After training the PINN, we estimate the time-varying coefficients and reconstruct the predicted signal using the\n",
    "observed past samples.  We compute the delay (tau) for phase-space reconstruction by finding the first local minimum\n",
    "of the autocorrelation function of the observed signal.  The embedding dimension for the 3D phase portrait is set\n",
    "to 3.  Finally, we compute and plot the 3D phase portrait for the observed and predicted signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "# Parameters for simulation\n",
    "N = 600\n",
    "p_order = 2\n",
    "W = 40\n",
    "P0 = 0.5\n",
    "freq1 = 1/150\n",
    "freq2 = 1/200\n",
    "\n",
    "# Define coefficient functions\n",
    "coeff_funcs = [lambda t: 0.6 + 0.35 * np.sin(2 * np.pi * (1/150) * t),\n",
    "               lambda t: -0.5 + 0.25 * np.cos(2 * np.pi * (1/200) * t)]\n",
    "\n",
    "# Simulate the signal and true coefficients\n",
    "x, coeffs_true = simulate_tvar_constant_power(N, p_order, W, P0, coeff_funcs,\n",
    "                                             noise_std=0.1, seed=42, c_max=5.0)\n",
    "\n",
    "# Train the PINN\n",
    "pinn_model, history = train_pinn(x, p_order, W, P0, n_epochs=200, lr=0.01,\n",
    "                                 lambda_energy=0.1, lambda_smooth=0.01,\n",
    "                                 h1=20, h2=20, seed=1, scale_coeff=0.5)\n",
    "\n",
    "# Predict coefficients for all time points\n",
    "norm_t = np.linspace(0.0, 1.0, N).reshape(-1, 1)\n",
    "coeff_pred_raw, _ = pinn_model.forward(norm_t)\n",
    "scale_coeff = 0.5\n",
    "coeff_pred = scale_coeff * coeff_pred_raw\n",
    "\n",
    "# Reconstruct predicted signal using predicted coefficients and observed past samples\n",
    "pred_signal = np.zeros(N)\n",
    "pred_signal[:p_order] = x[:p_order]\n",
    "for i in range(p_order, N):\n",
    "    coeffs = coeff_pred[i]\n",
    "    pred_signal[i] = coeffs[0] * x[i-1] + coeffs[1] * x[i-2]\n",
    "\n",
    "# Compute autocorrelation of the observed signal to estimate tau\n",
    "max_lag = 100\n",
    "acf = np.array([np.corrcoef(x[:-lag], x[lag:])[0,1] if lag > 0 else 1.0 for lag in range(max_lag)])\n",
    "mins = argrelextrema(acf, np.less)[0]\n",
    "tau = int(mins[0]) if len(mins) > 0 else 1\n",
    "\n",
    "embedding_dim = 3\n",
    "# Compute lag embedding for observed and predicted signals\n",
    "max_index = N - (embedding_dim - 1) * tau\n",
    "embedding_actual = np.column_stack([x[i: i + max_index] for i in range(0, embedding_dim * tau, tau)])\n",
    "embedding_pred = np.column_stack([pred_signal[i: i + max_index] for i in range(0, embedding_dim * tau, tau)])\n",
    "\n",
    "# Print tau and embedding dimension\n",
    "print(f\"Estimated time delay (tau): {tau}\")\n",
    "print(f\"Embedding dimension (p): {embedding_dim}\")\n",
    "\n",
    "# Plot the training loss components\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(history['total'], label='Total loss')\n",
    "plt.plot(history['ar'], label='AR residual')\n",
    "plt.plot(history['energy'], label='Energy penalty')\n",
    "plt.plot(history['smooth'], label='Smoothness penalty')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Loss components during training')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot true vs predicted coefficients\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(coeffs_true[:, 0], label='True a1')\n",
    "plt.plot(coeff_pred[:, 0], label='Predicted a1', linestyle='--')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Coefficient a1')\n",
    "plt.title('First AR coefficient')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(coeffs_true[:, 1], label='True a2')\n",
    "plt.plot(coeff_pred[:, 1], label='Predicted a2', linestyle='--')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Coefficient a2')\n",
    "plt.title('Second AR coefficient')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3D phase portrait\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Subsample for clarity\n",
    "idx = np.arange(0, embedding_actual.shape[0], 5)\n",
    "ax.plot(embedding_actual[idx, 0], embedding_actual[idx, 1], embedding_actual[idx, 2], label='Observed', alpha=0.7)\n",
    "ax.plot(embedding_pred[idx, 0], embedding_pred[idx, 1], embedding_pred[idx, 2], label='Predicted', alpha=0.7)\n",
    "ax.set_title(f'3D Phase Portrait (tau = {tau})')\n",
    "ax.set_xlabel('x(t)')\n",
    "ax.set_ylabel(f'x(t+{tau})')\n",
    "ax.set_zlabel(f'x(t+2*{tau})')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bda77",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated a complete workflow for simulating, training, and analysing a time-varying autoregressive\n",
    "process using a physics-informed neural network.  The code is organised into modular functions so that the PINN\n",
    "and the lag-embedding analysis can be applied to real data by replacing the simulated signal with an observed time\n",
    "series.  The estimated time delay (tau) and embedding dimension (p) provide the parameters needed to reconstruct\n",
    "a phase space in three dimensions and visualise the dynamical structure of the signal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
